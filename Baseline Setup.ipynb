{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"16W6PrOzmPKyPg4BpWtnG1chaZ3XbETaY","timestamp":1745029982181}],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9d7c36f5603b44c9b06e90e7a3250a71":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8b88324719724366a98135d8df8f61b5","IPY_MODEL_eab2cb85925a40f085c83a8f8264b72b","IPY_MODEL_c7b66e59110544ec8d8f4c5b1ade90cf"],"layout":"IPY_MODEL_46a4e756b1e74f6d92d1ab0c8d1e8f67"}},"8b88324719724366a98135d8df8f61b5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60c94f6e2d864701843257302818b37a","placeholder":"​","style":"IPY_MODEL_4d3aae3b66854948aaec8955771f9c0c","value":"Map: 100%"}},"eab2cb85925a40f085c83a8f8264b72b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_368c7f49a2e6430f845cf394328e918e","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2b0c76780ec446e9b177d6aff2e37158","value":5}},"c7b66e59110544ec8d8f4c5b1ade90cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9fd1e7b0b7994722ab450ce8cedf8276","placeholder":"​","style":"IPY_MODEL_72863f49349b40b1b8a13a7c7adf436a","value":" 5/5 [00:00&lt;00:00, 166.95 examples/s]"}},"46a4e756b1e74f6d92d1ab0c8d1e8f67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60c94f6e2d864701843257302818b37a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d3aae3b66854948aaec8955771f9c0c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"368c7f49a2e6430f845cf394328e918e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b0c76780ec446e9b177d6aff2e37158":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9fd1e7b0b7994722ab450ce8cedf8276":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72863f49349b40b1b8a13a7c7adf436a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90c29290b7054c458a3ee650d0811990":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f3f647ed47ee441a81bca1437c8f537c","IPY_MODEL_07a48bebff1c4a5583472d67452dacba","IPY_MODEL_d2215ec6abbd4931ba53107bb87e83cf"],"layout":"IPY_MODEL_b120160881b747ab80a621b383604459"}},"f3f647ed47ee441a81bca1437c8f537c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e935cf8a70940ee9c8c1fad715f2d26","placeholder":"​","style":"IPY_MODEL_7f2cf446ef0a454c9f57e6c672b6df50","value":"Map (num_proc=4): 100%"}},"07a48bebff1c4a5583472d67452dacba":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4745192455a145fba9b797a68448ef8c","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3e338f25faa040718848bc264d64e168","value":100}},"d2215ec6abbd4931ba53107bb87e83cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_251113e333ed4bc2bcdabb8acb33e642","placeholder":"​","style":"IPY_MODEL_8c9f3440d415442b916065f6164fbd6a","value":" 100/100 [00:00&lt;00:00, 116.94 examples/s]"}},"b120160881b747ab80a621b383604459":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e935cf8a70940ee9c8c1fad715f2d26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f2cf446ef0a454c9f57e6c672b6df50":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4745192455a145fba9b797a68448ef8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e338f25faa040718848bc264d64e168":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"251113e333ed4bc2bcdabb8acb33e642":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c9f3440d415442b916065f6164fbd6a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e15162901d244209705f1ae51978a39":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_026454c49fb448c78ce75ea769539fbe","IPY_MODEL_33c9d25d95cf4f9aa5734779e997f496","IPY_MODEL_c0ddd283f24940eca2603f52e3fe5dee"],"layout":"IPY_MODEL_9be3845711ba444ab682503134e1ff94"}},"026454c49fb448c78ce75ea769539fbe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2493901fc3384e36a8b93455288f5ed3","placeholder":"​","style":"IPY_MODEL_491d7ee66eaf4d0b9d3787a300e84d57","value":"Map (num_proc=4): 100%"}},"33c9d25d95cf4f9aa5734779e997f496":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d342818c70448d792e4e718c062f7a7","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cc77a4ab4a324f7a99b3309a3454d166","value":10}},"c0ddd283f24940eca2603f52e3fe5dee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c33319ae59449b2ba230b2f9b5e59cc","placeholder":"​","style":"IPY_MODEL_dda0f8d28970412dad7b5cd6c4b60224","value":" 10/10 [00:00&lt;00:00, 55.95 examples/s]"}},"9be3845711ba444ab682503134e1ff94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2493901fc3384e36a8b93455288f5ed3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"491d7ee66eaf4d0b9d3787a300e84d57":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d342818c70448d792e4e718c062f7a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc77a4ab4a324f7a99b3309a3454d166":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8c33319ae59449b2ba230b2f9b5e59cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dda0f8d28970412dad7b5cd6c4b60224":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4270b12d7c274cbcaba177e0e7f38581":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0e6a6f2ff9674148b6b09f7e3ee9c332","IPY_MODEL_b32309d694524f00897059bdec223202","IPY_MODEL_baf6703bccd14947855aa2d82f7f3783"],"layout":"IPY_MODEL_cf17358d26874a0fbf9aff1cffdbd748"}},"0e6a6f2ff9674148b6b09f7e3ee9c332":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d2615e802f94d9da48e953200923e93","placeholder":"​","style":"IPY_MODEL_8959f6a4d6bd43369b1ed7ffecaeb937","value":"Map (num_proc=4): 100%"}},"b32309d694524f00897059bdec223202":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_584f77d2a9294692942f9fe56f61091c","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_51729df8be074eeb86bb9ad04fd6f0d4","value":10}},"baf6703bccd14947855aa2d82f7f3783":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_87e0d50488d546d59a2156e5f707f590","placeholder":"​","style":"IPY_MODEL_e19af83eea8a49eca28cf769c28ec5d0","value":" 10/10 [00:00&lt;00:00, 55.93 examples/s]"}},"cf17358d26874a0fbf9aff1cffdbd748":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d2615e802f94d9da48e953200923e93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8959f6a4d6bd43369b1ed7ffecaeb937":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"584f77d2a9294692942f9fe56f61091c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51729df8be074eeb86bb9ad04fd6f0d4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"87e0d50488d546d59a2156e5f707f590":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e19af83eea8a49eca28cf769c28ec5d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c589499e6564b7e813d9af99e209141":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_144983a9738b4b1095886d26a5112f84","IPY_MODEL_512dac534ae14a1eb0d97352dc8e04dc","IPY_MODEL_44b97d4f1b4845229dab8eed92616a44"],"layout":"IPY_MODEL_cae1cf2b3a764e26aef92a1f5642310e"}},"144983a9738b4b1095886d26a5112f84":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e995d0a017e440afa0bba1aafba05814","placeholder":"​","style":"IPY_MODEL_3b584dfb501d431aafe27a174280c718","value":"Map (num_proc=3): 100%"}},"512dac534ae14a1eb0d97352dc8e04dc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1bb9a961d874b74bb2f078881c23988","max":200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_beb082069bc64ea5bf86d4966076803f","value":200}},"44b97d4f1b4845229dab8eed92616a44":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d7628191b5041a88d7e31bec81b9010","placeholder":"​","style":"IPY_MODEL_85acb66889974a618320a571f4287ddb","value":" 200/200 [00:00&lt;00:00, 140.17 examples/s]"}},"cae1cf2b3a764e26aef92a1f5642310e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e995d0a017e440afa0bba1aafba05814":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b584dfb501d431aafe27a174280c718":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d1bb9a961d874b74bb2f078881c23988":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"beb082069bc64ea5bf86d4966076803f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6d7628191b5041a88d7e31bec81b9010":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85acb66889974a618320a571f4287ddb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"799a63d56a554d6aafd64c7aed534c7f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a31c8bad3fc1403fb8b69bd764217b30","IPY_MODEL_945841ba4235485691d4d089d5352486","IPY_MODEL_212f90fdfe964a8ca76e4d6046e9d731"],"layout":"IPY_MODEL_925dcbd4437649b39403eccd34f4a4fd"}},"a31c8bad3fc1403fb8b69bd764217b30":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ba3943a4add49a29d466c6fc0e6284a","placeholder":"​","style":"IPY_MODEL_ea245f73ebbf4600b30bb015d0f82f7b","value":"Map (num_proc=3): 100%"}},"945841ba4235485691d4d089d5352486":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b5c3adc7c37480e805701344dca41d3","max":15,"min":0,"orientation":"horizontal","style":"IPY_MODEL_24a6c2da4c934cdf86fb1846ae080d38","value":15}},"212f90fdfe964a8ca76e4d6046e9d731":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8dcaee53b9d471ab0234f756845115c","placeholder":"​","style":"IPY_MODEL_3ce05e49f6c44aae99cd0b9df275c7ce","value":" 15/15 [00:00&lt;00:00, 18.80 examples/s]"}},"925dcbd4437649b39403eccd34f4a4fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ba3943a4add49a29d466c6fc0e6284a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea245f73ebbf4600b30bb015d0f82f7b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b5c3adc7c37480e805701344dca41d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24a6c2da4c934cdf86fb1846ae080d38":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b8dcaee53b9d471ab0234f756845115c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ce05e49f6c44aae99cd0b9df275c7ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b12cabf6f3e84f8999bccbe8b5b2cc89":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_26d5c3bf35274f75b1b8bc6bf5eac823","IPY_MODEL_47fba816b7a74207a15a1fb8d8ba246d","IPY_MODEL_04d906fbc65c4a32a7704e50264f0897"],"layout":"IPY_MODEL_025435c044744fa8bcf806bd6cca1148"}},"26d5c3bf35274f75b1b8bc6bf5eac823":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a38208d478324e04a55bb2bd2247f601","placeholder":"​","style":"IPY_MODEL_800b6e86437a451795802a5bd7f68ab6","value":"Map (num_proc=3): 100%"}},"47fba816b7a74207a15a1fb8d8ba246d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c846292126e641bcaf8fe584e12ce97c","max":15,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6aa6e39f7bc84d54a6a65bdc85b49469","value":15}},"04d906fbc65c4a32a7704e50264f0897":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fec8467f3824a2990d7b731fa4ba229","placeholder":"​","style":"IPY_MODEL_d34c5ebfd90640cf9976993556eb6916","value":" 15/15 [00:00&lt;00:00, 18.27 examples/s]"}},"025435c044744fa8bcf806bd6cca1148":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a38208d478324e04a55bb2bd2247f601":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"800b6e86437a451795802a5bd7f68ab6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c846292126e641bcaf8fe584e12ce97c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6aa6e39f7bc84d54a6a65bdc85b49469":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4fec8467f3824a2990d7b731fa4ba229":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d34c5ebfd90640cf9976993556eb6916":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42da0446dd694bb49363ef9b96b3417c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e52108467fd3427d9359323e5d44caed","IPY_MODEL_f45f8a818c5b4efa82683b5d3f083245","IPY_MODEL_aaac1d25fd6b40439eb21a4699f9f9d0"],"layout":"IPY_MODEL_38acaa3a679046e8b63edef454d2fd25"}},"e52108467fd3427d9359323e5d44caed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bcc48b7cc8e46b182523e37c1b115c1","placeholder":"​","style":"IPY_MODEL_e7e2a4a2a7614244a0965bc09c5f0c1d","value":"Map (num_proc=4): 100%"}},"f45f8a818c5b4efa82683b5d3f083245":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d75a68736f54237a28db7514e2c26b3","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7b0d03c3608e4b49a1988dbbc5c72dfe","value":20}},"aaac1d25fd6b40439eb21a4699f9f9d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7857815fb0fa4e95947ddbcbc1e715bb","placeholder":"​","style":"IPY_MODEL_5ac56f1886ba4423a3be95554f045d1a","value":" 20/20 [00:00&lt;00:00, 11.44 examples/s]"}},"38acaa3a679046e8b63edef454d2fd25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bcc48b7cc8e46b182523e37c1b115c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7e2a4a2a7614244a0965bc09c5f0c1d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d75a68736f54237a28db7514e2c26b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b0d03c3608e4b49a1988dbbc5c72dfe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7857815fb0fa4e95947ddbcbc1e715bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ac56f1886ba4423a3be95554f045d1a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"50f61df0854948cba5d35e1c250c6855":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3e66c871d74e487990a8041a29f9ffc0","IPY_MODEL_90759a93e53d464ab23a443f88de7425","IPY_MODEL_2de28ce9cb27484ebca08f696d4c912d"],"layout":"IPY_MODEL_27a81fe9feae498db16faf0666deba95"}},"3e66c871d74e487990a8041a29f9ffc0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8ca791115a64be59cefc07cf881e8c3","placeholder":"​","style":"IPY_MODEL_bb0fab66fd7e49c5ada1a84d0b6679e4","value":"Map (num_proc=3): 100%"}},"90759a93e53d464ab23a443f88de7425":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc088e21809b436fa42ee52e474bd0a1","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5909ab68d5ab4152b6ec7ec38f66147b","value":3}},"2de28ce9cb27484ebca08f696d4c912d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d563f474ac404a58ad43a37b645fe1a1","placeholder":"​","style":"IPY_MODEL_4bbbea0a1c244f96ac3d27cfeb176e29","value":" 3/3 [00:00&lt;00:00,  3.14 examples/s]"}},"27a81fe9feae498db16faf0666deba95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8ca791115a64be59cefc07cf881e8c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb0fab66fd7e49c5ada1a84d0b6679e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc088e21809b436fa42ee52e474bd0a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5909ab68d5ab4152b6ec7ec38f66147b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d563f474ac404a58ad43a37b645fe1a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bbbea0a1c244f96ac3d27cfeb176e29":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac03f53a67f74edd90a52c9fab6df6f7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5219da212261437ea8d02b87e6c3f2db","IPY_MODEL_e0935c6b38d54f3d86cb72fa4fa98ed1","IPY_MODEL_2c19ac68bbe44826ad683fb896940b26"],"layout":"IPY_MODEL_fd16e2f150314274a0910cbd6bc43e29"}},"5219da212261437ea8d02b87e6c3f2db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1164a9a8b9c4780869f6a6d1387f782","placeholder":"​","style":"IPY_MODEL_7b9f3c72c9994db580dc3d2139b4f86d","value":"Map (num_proc=3): 100%"}},"e0935c6b38d54f3d86cb72fa4fa98ed1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_31bb4709c7874555974540a6cf1ca4a8","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ddec86b0ee414930ac2e20f5a3a6510c","value":3}},"2c19ac68bbe44826ad683fb896940b26":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c55170974093467e97d4481fb634e2da","placeholder":"​","style":"IPY_MODEL_23123636ffce497abef1cdf37322af7b","value":" 3/3 [00:00&lt;00:00,  3.13 examples/s]"}},"fd16e2f150314274a0910cbd6bc43e29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1164a9a8b9c4780869f6a6d1387f782":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b9f3c72c9994db580dc3d2139b4f86d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31bb4709c7874555974540a6cf1ca4a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddec86b0ee414930ac2e20f5a3a6510c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c55170974093467e97d4481fb634e2da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23123636ffce497abef1cdf37322af7b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# @title Baseline Model:T5 base - Set-Up\n","# !pip install transformers datasets rouge-score\n","\n","import torch\n","import numpy as np\n","from datasets import load_dataset\n","from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n","from torch.utils.data import Dataset\n","from rouge_score import rouge_scorer\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-dOdOqOL7cSW","executionInfo":{"status":"ok","timestamp":1745031434859,"user_tz":300,"elapsed":6,"user":{"displayName":"Nikhitha Gollamudi","userId":"11701806671984832308"}},"outputId":"ebefeea5-2bcc-4274-b51b-f84f2cb6b03c","cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","source":["# @title Loading Datatset, Tokenizer and Model\n","dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n","print(f\"Dataset loaded - Train: {len(dataset['train'])}, Validation: {len(dataset['validation'])}, Test: {len(dataset['test'])}\")\n","\n","# Load tokenizer and model\n","tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n","model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n","model = model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p9wRC5hG7us8","executionInfo":{"status":"ok","timestamp":1745031449040,"user_tz":300,"elapsed":12160,"user":{"displayName":"Nikhitha Gollamudi","userId":"11701806671984832308"}},"outputId":"206831ef-2c13-447d-f51d-63a433f4b53f","cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset loaded - Train: 287113, Validation: 13368, Test: 11490\n"]}]},{"cell_type":"code","source":["# @title SummarizationDataset Class\n","class SummarizationDataset(Dataset):\n","    def __init__(self, data, tokenizer, max_input_len=512, max_target_len=128):\n","        self.inputs = [\"summarize: \" + x for x in data[\"article\"]]\n","        self.targets = data[\"highlights\"]\n","        self.tokenizer = tokenizer\n","        self.max_input_len = max_input_len\n","        self.max_target_len = max_target_len\n","\n","    def __len__(self):\n","        return len(self.inputs)\n","\n","    def __getitem__(self, idx):\n","        input_enc = self.tokenizer(\n","            self.inputs[idx], padding=\"max_length\", truncation=True,\n","            max_length=self.max_input_len, return_tensors=\"pt\"\n","        )\n","        target_enc = self.tokenizer(\n","            self.targets[idx], padding=\"max_length\", truncation=True,\n","            max_length=self.max_target_len, return_tensors=\"pt\"\n","        )\n","        input_ids = input_enc[\"input_ids\"].squeeze()\n","        attention_mask = input_enc[\"attention_mask\"].squeeze()\n","        labels = target_enc[\"input_ids\"].squeeze()\n","        labels[labels == tokenizer.pad_token_id] = -100  # Ignore pad tokens in loss\n","\n","        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}"],"metadata":{"id":"Xu7g_zPK8AZ7","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Create Training and Evaluation Datatsets\n","train_dataset = SummarizationDataset(dataset[\"train\"].select(range(2000)), tokenizer)\n","val_dataset = SummarizationDataset(dataset[\"validation\"].select(range(500)), tokenizer)\n","\n","print(f\"Training samples: {len(train_dataset)}\")\n","print(f\"Validation samples: {len(val_dataset)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wuutKX__8ckz","executionInfo":{"status":"ok","timestamp":1745031449068,"user_tz":300,"elapsed":18,"user":{"displayName":"Nikhitha Gollamudi","userId":"11701806671984832308"}},"outputId":"ca33ee37-635b-4200-9d08-54ddd7028478","cellView":"form","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training samples: 2000\n","Validation samples: 500\n"]}]},{"cell_type":"code","source":["# @title Evaluation metrics\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","\n","    # Replace -100 in the labels as we can't decode them\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Rouge scores\n","    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","\n","    rouge1 = rouge2 = rougeL = 0.0\n","    for pred, label in zip(decoded_preds, decoded_labels):\n","        scores = scorer.score(label, pred)\n","        rouge1 += scores['rouge1'].fmeasure\n","        rouge2 += scores['rouge2'].fmeasure\n","        rougeL += scores['rougeL'].fmeasure\n","\n","    # Average scores\n","    count = len(decoded_preds)\n","    return {\n","        'rouge1': rouge1 / count,\n","        'rouge2': rouge2 / count,\n","        'rougeL': rougeL / count\n","    }"],"metadata":{"id":"O5qNv7-2804v","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Configure Training\n","training_args = TrainingArguments(\n","    output_dir=\"./t5-news-summarizer\",\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=1,\n","    learning_rate=3e-4,\n","    # Removed: evaluation_strategy=\"epoch\",\n","    # Removed: save_strategy=\"epoch\",\n","    save_steps=500,       # Save every 500 steps instead\n","    eval_steps=500,       # Evaluate every 500 steps\n","    save_total_limit=2,   # Keep only the 2 most recent checkpoints\n","    weight_decay=0.01,\n","    logging_steps=50,\n","    logging_dir=\"./logs\",\n","    report_to=\"none\"\n",")"],"metadata":{"id":"XvI8vapv87kn","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Train Model\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=compute_metrics\n",")\n","\n","print(\"Starting training...\")\n","trainer.train()\n","print(\"Training complete!\")\n","\n","# Save fine-tuned model\n","model.save_pretrained(\"./t5-news-summarizer\")\n","tokenizer.save_pretrained(\"./t5-news-summarizer\")\n","print(\"Model and tokenizer saved!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"id":"HiyK8KiE9BEK","executionInfo":{"status":"ok","timestamp":1745032027623,"user_tz":300,"elapsed":346692,"user":{"displayName":"Nikhitha Gollamudi","userId":"11701806671984832308"}},"outputId":"ed8c5734-1bdf-4e0a-ee98-08fff5b166a6","cellView":"form","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting training...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [250/250 05:41, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>50</td>\n","      <td>1.747200</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>1.689500</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>1.661200</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>1.670800</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>1.639600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Training complete!\n","Model and tokenizer saved!\n"]}]},{"cell_type":"code","source":["# @title Summary Generation Function\n","def generate_summary(text, model, tokenizer, max_input_length=512, max_target_length=128):\n","    # Get the device that the model is on\n","    device = next(model.parameters()).device\n","\n","    # Make sure input_ids are on the same device as the model\n","    input_ids = tokenizer(\"summarize: \" + text, return_tensors=\"pt\",\n","                         truncation=True, max_length=max_input_length).input_ids.to(device)\n","\n","    # Generate summary\n","    output_ids = model.generate(input_ids, max_length=max_target_length,\n","                              num_beams=4, early_stopping=True)\n","\n","    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n"],"metadata":{"id":"VJitxXls9QcE","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Samples generation\n","print(\"Generating sample summary...\")\n","example_article = dataset[\"test\"][0][\"article\"]\n","print(\"\\nOriginal Article:\\n\", example_article[:500], \"...\\n\")\n","generated_summary = generate_summary(example_article, model, tokenizer)\n","print(\"\\nGenerated Summary:\\n\", generated_summary)\n","print(\"\\nReference Summary:\\n\", dataset[\"test\"][0][\"highlights\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6KWADzOx9TTq","executionInfo":{"status":"ok","timestamp":1745032162176,"user_tz":300,"elapsed":1968,"user":{"displayName":"Nikhitha Gollamudi","userId":"11701806671984832308"}},"outputId":"465c69e9-789e-44a8-b9b2-c5c4accbf297","cellView":"form","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generating sample summary...\n","\n","Original Article:\n"," (CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, includin ...\n","\n","\n","Generated Summary:\n"," Palestinian Authority officially becomes 123rd member of the International Criminal Court . The move gives the court jurisdiction over alleged crimes committed in Palestinian territories . Israel and the United States, neither of which is an ICC member, oppose the move .\n","\n","Reference Summary:\n"," Membership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June .\n","Israel and the United States opposed the move, which could open the door to war crimes investigations against Israelis .\n"]}]},{"cell_type":"code","source":["# @title Comprehensive Evaluation Function\n","def evaluate_summaries(dataset, model, tokenizer, num_samples=100):\n","    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","    rouge_scores = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n","\n","    print(f\"Evaluating on {min(num_samples, len(dataset))} samples...\")\n","\n","    for i in range(min(num_samples, len(dataset))):\n","        if i % 10 == 0:\n","            print(f\"Processing sample {i}...\")\n","\n","        article = dataset[i][\"article\"]\n","        reference = dataset[i][\"highlights\"]\n","        generated = generate_summary(article, model, tokenizer)\n","\n","        scores = scorer.score(reference, generated)\n","        for key in rouge_scores:\n","            rouge_scores[key] += scores[key].fmeasure\n","\n","    # Average scores\n","    for key in rouge_scores:\n","        rouge_scores[key] /= min(num_samples, len(dataset))\n","\n","    print(f\"Evaluation complete!\")\n","    return rouge_scores"],"metadata":{"id":"6g2PJLIQ9cYD","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Evaluation\n","test_dataset = dataset[\"test\"].select(range(100))\n","test_scores = evaluate_summaries(test_dataset, model, tokenizer)\n","print(f\"Test set ROUGE scores: {test_scores}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eVAd4O7s9hih","executionInfo":{"status":"ok","timestamp":1745031360082,"user_tz":300,"elapsed":137852,"user":{"displayName":"Nikhitha Gollamudi","userId":"11701806671984832308"}},"outputId":"939e76ee-539b-4c6a-a87b-a66e5e3cf259","collapsed":true,"cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluating on 100 samples...\n","Processing sample 0...\n","Processing sample 10...\n","Processing sample 20...\n","Processing sample 30...\n","Processing sample 40...\n","Processing sample 50...\n","Processing sample 60...\n","Processing sample 70...\n","Processing sample 80...\n","Processing sample 90...\n","Evaluation complete!\n","Test set ROUGE scores: {'rouge1': 0.3301861812648006, 'rouge2': 0.13454597845211605, 'rougeL': 0.252344467616384}\n"]}]},{"cell_type":"code","source":["# @title Bart-Base pipeline\n","# --------------------------------------------\n","!pip install -q transformers==4.51.3 datasets evaluate rouge-score nltk accelerate peft trl"],"metadata":{"cellView":"form","id":"0pEPFP_xVjbq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title pipeline-1\n","import torch\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n","from datasets import load_dataset, DatasetDict\n","import evaluate\n","import numpy as np\n","\n","# 1. CONFIG\n","class SFTConfig:\n","    def __init__(self):\n","        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        self.max_samples = {\"train\": 25, \"validation\": 5, \"test\": 5}\n","        self.batch_size = 2 if self.device == \"cpu\" else 8\n","        self.max_input_length = 512\n","        self.model_name = \"facebook/bart-base\"\n","        self.epochs = 1\n","        self.save_steps = 50\n","        self.max_output_length = 128  # Default value\n","\n","# 2. DATA MANAGER (FIXED)\n","class SFDataManager:\n","    def __init__(self, config):\n","        self.config = config\n","        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n","        self.rouge = evaluate.load(\"rouge\")\n","\n","    def load_data(self):\n","        ds = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n","        data = DatasetDict({\n","            k: ds[k].shuffle().select(range(self.config.max_samples[k]))\n","            for k in [\"train\", \"validation\", \"test\"]\n","        })\n","\n","        # SAFE length calculation\n","        if len(data[\"train\"][\"highlights\"]) > 0:\n","            lens = [len(self.tokenizer.tokenize(x)) for x in data[\"train\"][\"highlights\"][:10]]\n","            avg_len = sum(lens)/max(1, len(lens))\n","            self.config.max_output_length = min(int(avg_len * 2), 128)\n","        return data\n","\n","    def tokenize(self, dataset):\n","        def process(examples):\n","            model_inputs = self.tokenizer(\n","                examples[\"article\"],\n","                max_length=self.config.max_input_length,\n","                truncation=True,\n","                padding=\"max_length\"\n","            )\n","            labels = self.tokenizer(\n","                text_target=examples[\"highlights\"],\n","                max_length=self.config.max_output_length,\n","                truncation=True,\n","                padding=\"max_length\"\n","            )\n","            model_inputs[\"labels\"] = labels[\"input_ids\"]\n","            return model_inputs\n","\n","        return dataset.map(process, batched=True, batch_size=4)\n","\n","    def compute_metrics(self, eval_preds):\n","        preds, labels = eval_preds\n","        decoded_preds = self.tokenizer.batch_decode(preds, skip_special_tokens=True)\n","        labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)\n","        decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n","        return self.rouge.compute(\n","            predictions=decoded_preds,\n","            references=decoded_labels,\n","            use_stemmer=True\n","        )\n","\n","# 3. TRAINER (FIXED)\n","class SFTrainer:\n","    def __init__(self, config):\n","        self.config = config\n","        self.data_mgr = SFDataManager(config)\n","        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n","            config.model_name,\n","            torch_dtype=torch.float32  # Safer for CPU\n","        ).to(config.device)\n","        self.data_collator = DataCollatorForSeq2Seq(\n","            self.data_mgr.tokenizer,\n","            model=self.model,\n","            padding=True\n","        )\n","\n","    def train(self, tokenized_data):\n","        args = Seq2SeqTrainingArguments(\n","            output_dir=\"./saved_model\",\n","            per_device_train_batch_size=self.config.batch_size,\n","            per_device_eval_batch_size=2,\n","            predict_with_generate=True,\n","            eval_strategy=\"steps\",\n","            eval_steps=25,\n","            logging_steps=10,\n","            save_strategy=\"steps\",\n","            save_steps=self.config.save_steps,\n","            num_train_epochs=self.config.epochs,\n","            report_to=\"none\",\n","            remove_unused_columns=True,\n","            generation_max_length=self.config.max_output_length,\n","            generation_num_beams=2\n","        )\n","\n","        trainer = Seq2SeqTrainer(\n","            model=self.model,\n","            args=args,\n","            train_dataset=tokenized_data[\"train\"],\n","            eval_dataset=tokenized_data[\"validation\"],\n","            tokenizer=self.data_mgr.tokenizer,\n","            compute_metrics=self.data_mgr.compute_metrics,\n","            data_collator=self.data_collator\n","        )\n","\n","        trainer.train()\n","        return trainer\n","\n","# 4. EXECUTION\n","def main():\n","    print(f\"Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n","\n","    print(\"Loading data...\")\n","    config = SFTConfig()\n","    data_mgr = SFDataManager(config)\n","    raw_data = data_mgr.load_data()\n","\n","    print(f\"Using max_output_length = {config.max_output_length}\")\n","\n","    print(\"Tokenizing...\")\n","    tokenized_data = data_mgr.tokenize(raw_data)\n","\n","    print(\"Fine-tuning...\")\n","    trainer = SFTrainer(config)\n","    trainer.train(tokenized_data)\n","\n","    print(\"\\nSample generation:\")\n","    test_sample = raw_data[\"test\"][0][\"article\"][:config.max_input_length]\n","    inputs = data_mgr.tokenizer(\n","        test_sample,\n","        max_length=config.max_input_length,\n","        truncation=True,\n","        padding=\"max_length\",\n","        return_tensors=\"pt\"\n","    ).to(config.device)\n","\n","    outputs = trainer.model.generate(\n","        **inputs,\n","        max_length=config.max_output_length,\n","        num_beams=2,\n","        early_stopping=True\n","    )\n","\n","    print(\"\\nInput:\", test_sample[:200] + \"...\")\n","    print(\"\\nGenerated:\", data_mgr.tokenizer.decode(outputs[0], skip_special_tokens=True))\n","    print(\"\\nReference:\", raw_data[\"test\"][0][\"highlights\"])\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":475,"referenced_widgets":["9d7c36f5603b44c9b06e90e7a3250a71","8b88324719724366a98135d8df8f61b5","eab2cb85925a40f085c83a8f8264b72b","c7b66e59110544ec8d8f4c5b1ade90cf","46a4e756b1e74f6d92d1ab0c8d1e8f67","60c94f6e2d864701843257302818b37a","4d3aae3b66854948aaec8955771f9c0c","368c7f49a2e6430f845cf394328e918e","2b0c76780ec446e9b177d6aff2e37158","9fd1e7b0b7994722ab450ce8cedf8276","72863f49349b40b1b8a13a7c7adf436a"]},"id":"67FE7HDNEgxD","outputId":"5893aed0-3121-4a63-c3bc-96475171b909","executionInfo":{"status":"ok","timestamp":1746380081480,"user_tz":300,"elapsed":85101,"user":{"displayName":"Akhil Mavallapalli","userId":"02508402536572482827"}},"collapsed":true,"cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","Loading data...\n","Using max_output_length = 128\n","Tokenizing...\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/5 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d7c36f5603b44c9b06e90e7a3250a71"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Fine-tuning...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-11-69a4b07638fc>:103: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n","  trainer = Seq2SeqTrainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [13/13 01:04, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Sample generation:\n","\n","Input: This is the dramatic moment a golden retriever had to be rescued from the water after being swept half a mile out to sea while playing on the shoreline. Ten-year-old Martha had been with her owner on ...\n","\n","Generated: This is the dramatic moment a golden retriever was swept out to sea after being swept into the sea by a strong currentNew Brighton RNLI crew spot golden retrievate Martha in the water after she was swept to the water in a strong position. Ten-year-old Martha had been with her owner on the beach in Leasowe in Merseyside and was paddling in the strong current. And with the strength of the outgoing tide, the dog was rapidly swept out of the water. But when she was caught out by the current, she was rescued and her owner launched an unsuccessful rescue attempt. New Brighton RN\n","\n","Reference: Martha, aged 10, had been paddling in the water when she was swept out .\n","Her owner launched their own rescue but was unable to grab the pet .\n","RNLI eventually found the golden retriever who was cold and shivering .\n","Brought her back to the shore after managing to grab her by the collar .\n"]}]},{"cell_type":"code","source":["# @title imports for code 2\n","# in a notebook cell, run:\n","!pip install -U datasets fsspec huggingface-hub\n","!pip install --upgrade evaluate datasets==2.13.4\n","!pip install --upgrade datasets\n","!pip install git+https://github.com/huggingface/evaluate.git\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"cellView":"form","collapsed":true,"id":"9wRXXp8m4xVQ","executionInfo":{"status":"ok","timestamp":1746388962009,"user_tz":300,"elapsed":13865,"user":{"displayName":"Akhil Mavallapalli","userId":"02508402536572482827"}},"outputId":"c002e1a2-2310-4504-a8ce-a9271156c026"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.0)\n","Collecting fsspec\n","  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.14)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.13.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.0)\n","Collecting evaluate\n","  Using cached evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement datasets==2.13.4 (from versions: 0.0.9, 1.0.0, 1.0.1, 1.0.2, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 1.2.0, 1.2.1, 1.3.0, 1.4.0, 1.4.1, 1.5.0, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.2, 1.13.3, 1.14.0, 1.15.0, 1.15.1, 1.16.0, 1.16.1, 1.17.0, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 2.0.0, 2.1.0, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.3.2, 2.4.0, 2.5.0, 2.5.1, 2.5.2, 2.6.0, 2.6.1, 2.6.2, 2.7.0, 2.7.1, 2.8.0, 2.9.0, 2.10.0, 2.10.1, 2.11.0, 2.12.0, 2.13.0, 2.13.1, 2.13.2, 2.14.0, 2.14.1, 2.14.2, 2.14.3, 2.14.4, 2.14.5, 2.14.6, 2.14.7, 2.15.0, 2.16.0, 2.16.1, 2.17.0, 2.17.1, 2.18.0, 2.19.0, 2.19.1, 2.19.2, 2.20.0, 2.21.0, 3.0.0, 3.0.1, 3.0.2, 3.1.0, 3.2.0, 3.3.0, 3.3.1, 3.3.2, 3.4.0, 3.4.1, 3.5.0, 3.5.1)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for datasets==2.13.4\u001b[0m\u001b[31m\n","\u001b[0mRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.14)\n","Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Collecting git+https://github.com/huggingface/evaluate.git\n","  Cloning https://github.com/huggingface/evaluate.git to /tmp/pip-req-build-sibpe7hl\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/evaluate.git /tmp/pip-req-build-sibpe7hl\n","  Resolved https://github.com/huggingface/evaluate.git to commit 5aa3982a9a8c86e506860e381d428a64b0cce73b\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.4.dev0) (3.5.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.4.dev0) (1.26.4)\n","Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.4.dev0) (0.3.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.4.dev0) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.4.dev0) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.4.dev0) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.4.dev0) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.4.dev0) (0.70.14)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate==0.4.4.dev0) (2025.3.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.4.dev0) (0.30.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.4.dev0) (25.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate==0.4.4.dev0) (3.18.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate==0.4.4.dev0) (18.1.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate==0.4.4.dev0) (3.11.15)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate==0.4.4.dev0) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate==0.4.4.dev0) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate==0.4.4.dev0) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate==0.4.4.dev0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate==0.4.4.dev0) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate==0.4.4.dev0) (2025.4.26)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate==0.4.4.dev0) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate==0.4.4.dev0) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate==0.4.4.dev0) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.4.dev0) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.4.dev0) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.4.dev0) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.4.dev0) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.4.dev0) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.4.dev0) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.4.dev0) (1.20.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate==0.4.4.dev0) (1.17.0)\n","Building wheels for collected packages: evaluate\n","  Building wheel for evaluate (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for evaluate: filename=evaluate-0.4.4.dev0-py3-none-any.whl size=84152 sha256=e11c70dde51018c2270bf9ffdab2c2a63abb7e47baccfa2490744d5785887a57\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-b4sh8j22/wheels/db/8a/54/0a6d3e485443050ed53ad322c3983b47f12614ec144a42cab4\n","Successfully built evaluate\n","Installing collected packages: evaluate\n","  Attempting uninstall: evaluate\n","    Found existing installation: evaluate 0.4.0\n","    Uninstalling evaluate-0.4.0:\n","      Successfully uninstalled evaluate-0.4.0\n","Successfully installed evaluate-0.4.4.dev0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["evaluate"]},"id":"6ed70426d37c452ca7c249dc5d265194"}},"metadata":{}}]},{"cell_type":"code","source":["# pip install transformers datasets evaluate tqdm nltk\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import logging\n","from transformers import logging as hf_logging\n","import datasets, evaluate\n","\n","# ─── Silence library logs ────────────────────────────────────────────────────\n","hf_logging.set_verbosity_error()\n","datasets.logging.set_verbosity_error()\n","evaluate.logging.set_verbosity_error()\n","logging.getLogger(\"nltk\").setLevel(logging.ERROR)\n","logging.getLogger(\"transformers.generation_utils\").setLevel(logging.ERROR)\n","\n","import torch\n","import numpy as np\n","from datasets import load_dataset, DatasetDict\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSeq2SeqLM,\n","    DataCollatorForSeq2Seq,\n","    Seq2SeqTrainingArguments,\n","    Seq2SeqTrainer,\n",")\n","\n","# ─── CONFIG (customize here) ────────────────────────────────────────────────\n","MODEL_NAME         = \"facebook/bart-large-cnn\"\n","OUTPUT_DIR         = \"./bart_cnn_sum\"\n","BATCH_SIZE         = 8\n","NUM_EPOCHS         = 3\n","MAX_INPUT_LENGTH   = 512\n","MAX_OUTPUT_LENGTH  = 142\n","\n","# Set to an int to debug on a subset, or to None to use the full split:\n","TRAIN_SIZE = 100\n","VAL_SIZE   = 10\n","TEST_SIZE  = 10\n","NUM_EPOCHS = 3\n","BATCH_SIZE = 16\n","\n","# Logging / checkpoint intervals (in steps)\n","LOG_STEPS        = 50\n","EVAL_STEPS       = 200\n","SAVE_STEPS       = 200\n","SAVE_TOTAL_LIMIT = 3\n","\n","# ─── DEVICE ────────────────────────────────────────────────────────────────\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# ─── METRICS ───────────────────────────────────────────────────────────────\n","rouge  = evaluate.load(\"rouge\")\n","meteor = evaluate.load(\"meteor\")\n","\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","    decoded_preds  = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    r = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    m = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n","    return {\n","        \"rouge1\": round(r[\"rouge1\"],4),\n","        \"rouge2\": round(r[\"rouge2\"],4),\n","        \"rougeL\": round(r[\"rougeL\"],4),\n","        \"meteor\": round(m[\"meteor\"],4),\n","    }\n","\n","# ─── LOAD & SAMPLE ─────────────────────────────────────────────────────────\n","print(\"Loading CNN/DailyMail 3.0.0…\")\n","raw = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n","\n","def sample_split(split, n):\n","    ds = raw[split]\n","    return ds.shuffle(seed=42).select(range(n)) if n is not None else ds\n","\n","data = DatasetDict({\n","    \"train\":      sample_split(\"train\",      TRAIN_SIZE),\n","    \"validation\": sample_split(\"validation\", VAL_SIZE),\n","    \"test\":       sample_split(\"test\",       TEST_SIZE),\n","})\n","print(f\"  → train={len(data['train'])}, val={len(data['validation'])}, test={len(data['test'])}\")\n","\n","# ─── TOKENIZER & PREPROCESS ─────────────────────────────────────────────────\n","print(\"Initializing tokenizer & tokenizing…\")\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","\n","def preprocess(batch):\n","    inputs = tokenizer(\n","        batch[\"article\"],\n","        max_length=MAX_INPUT_LENGTH,\n","        truncation=True,\n","        padding=\"max_length\"\n","    )\n","    labels = tokenizer(\n","        batch[\"highlights\"],\n","        max_length=MAX_OUTPUT_LENGTH,\n","        truncation=True,\n","        padding=\"max_length\"\n","    ).input_ids\n","    inputs[\"labels\"] = labels\n","    return inputs\n","\n","# parallelize over 4 cores\n","tokenized = data.map(\n","    preprocess,\n","    batched=True,\n","    num_proc=4,\n","    remove_columns=[\"article\",\"highlights\"]\n",")\n","\n","# ─── MODEL & COLLATOR ───────────────────────────────────────────────────────\n","print(f\"Loading model to {device}…\")\n","model     = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\n","collator  = DataCollatorForSeq2Seq(tokenizer, model=model)\n","\n","# ─── TRAINING ARGS ──────────────────────────────────────────────────────────\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir               = OUTPUT_DIR,\n","    num_train_epochs         = NUM_EPOCHS,\n","    per_device_train_batch_size = BATCH_SIZE,\n","    per_device_eval_batch_size  = BATCH_SIZE,\n","\n","    logging_strategy         = \"steps\",\n","    logging_steps            = LOG_STEPS,\n","    logging_first_step       = True,\n","\n","    evaluation_strategy      = \"steps\",\n","    eval_steps               = EVAL_STEPS,\n","\n","    save_strategy            = \"steps\",\n","    save_steps               = SAVE_STEPS,\n","    save_total_limit         = SAVE_TOTAL_LIMIT,\n","\n","    disable_tqdm             = True,\n","    predict_with_generate    = True,\n","    fp16                     = (device.type==\"cuda\"),\n","    load_best_model_at_end   = True,\n","    metric_for_best_model    = \"rougeL\",\n","    greater_is_better        = True,\n","    report_to                = \"none\",\n",")\n","\n","# ─── TRAINER ────────────────────────────────────────────────────────────────\n","trainer = Seq2SeqTrainer(\n","    model           = model,\n","    args            = training_args,\n","    train_dataset   = tokenized[\"train\"],\n","    eval_dataset    = tokenized[\"validation\"],\n","    tokenizer       = tokenizer,\n","    data_collator   = collator,\n","    compute_metrics = compute_metrics,\n",")\n","\n","# ─── TRAIN & RESUME ─────────────────────────────────────────────────────────\n","print(\"▶ Fine-tuning started…\")\n","trainer.train()\n","# If you restart the script later, pick up from the last checkpoint with:\n","#   trainer.train(resume_from_checkpoint=True)\n","\n","# ─── FINAL EVALUATION ───────────────────────────────────────────────────────\n","print(\"\\n== Validation Metrics ==\")\n","val_metrics = trainer.evaluate(tokenized[\"validation\"])\n","print({k: val_metrics[k] for k in sorted(val_metrics) if k.startswith(\"eval_\")})\n","\n","print(\"\\n== Test Metrics ==\")\n","test_metrics = trainer.evaluate(tokenized[\"test\"])\n","print({k: test_metrics[k] for k in sorted(test_metrics) if k.startswith(\"eval_\")})\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["90c29290b7054c458a3ee650d0811990","f3f647ed47ee441a81bca1437c8f537c","07a48bebff1c4a5583472d67452dacba","d2215ec6abbd4931ba53107bb87e83cf","b120160881b747ab80a621b383604459","1e935cf8a70940ee9c8c1fad715f2d26","7f2cf446ef0a454c9f57e6c672b6df50","4745192455a145fba9b797a68448ef8c","3e338f25faa040718848bc264d64e168","251113e333ed4bc2bcdabb8acb33e642","8c9f3440d415442b916065f6164fbd6a","1e15162901d244209705f1ae51978a39","026454c49fb448c78ce75ea769539fbe","33c9d25d95cf4f9aa5734779e997f496","c0ddd283f24940eca2603f52e3fe5dee","9be3845711ba444ab682503134e1ff94","2493901fc3384e36a8b93455288f5ed3","491d7ee66eaf4d0b9d3787a300e84d57","9d342818c70448d792e4e718c062f7a7","cc77a4ab4a324f7a99b3309a3454d166","8c33319ae59449b2ba230b2f9b5e59cc","dda0f8d28970412dad7b5cd6c4b60224","4270b12d7c274cbcaba177e0e7f38581","0e6a6f2ff9674148b6b09f7e3ee9c332","b32309d694524f00897059bdec223202","baf6703bccd14947855aa2d82f7f3783","cf17358d26874a0fbf9aff1cffdbd748","3d2615e802f94d9da48e953200923e93","8959f6a4d6bd43369b1ed7ffecaeb937","584f77d2a9294692942f9fe56f61091c","51729df8be074eeb86bb9ad04fd6f0d4","87e0d50488d546d59a2156e5f707f590","e19af83eea8a49eca28cf769c28ec5d0"]},"id":"xlOTbzulpfb1","outputId":"1ad1ab99-3650-4147-b4bb-96fac47f0f5e","executionInfo":{"status":"ok","timestamp":1746396381035,"user_tz":300,"elapsed":1329105,"user":{"displayName":"Akhil Mavallapalli","userId":"02508402536572482827"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Loading CNN/DailyMail 3.0.0…\n","  → train=100, val=10, test=10\n","Initializing tokenizer & tokenizing…\n"]},{"output_type":"display_data","data":{"text/plain":["Map (num_proc=4):   0%|          | 0/100 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90c29290b7054c458a3ee650d0811990"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map (num_proc=4):   0%|          | 0/10 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e15162901d244209705f1ae51978a39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map (num_proc=4):   0%|          | 0/10 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4270b12d7c274cbcaba177e0e7f38581"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loading model to cpu…\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id. If id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running training *****\n","  Num examples = 100\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 21\n","  Number of trainable parameters = 406290432\n","You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"stream","name":"stdout","text":["▶ Fine-tuning started…\n","{'loss': 7.2474, 'learning_rate': 4.761904761904762e-05, 'epoch': 0.14}\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id. If id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 16\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"min_length\": 56,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["{'train_runtime': 1197.7744, 'train_samples_per_second': 0.25, 'train_steps_per_second': 0.018, 'train_loss': 2.7065482366652716, 'epoch': 3.0}\n","\n","== Validation Metrics ==\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id. If id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 10\n","  Batch size = 16\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"min_length\": 56,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.368355631828308, 'eval_rouge1': 0.4014, 'eval_rouge2': 0.1555, 'eval_rougeL': 0.2687, 'eval_meteor': 0.3281, 'eval_runtime': 49.5882, 'eval_samples_per_second': 0.202, 'eval_steps_per_second': 0.02, 'epoch': 3.0}\n","{'eval_loss': 1.368355631828308, 'eval_meteor': 0.3281, 'eval_rouge1': 0.4014, 'eval_rouge2': 0.1555, 'eval_rougeL': 0.2687, 'eval_runtime': 49.5882, 'eval_samples_per_second': 0.202, 'eval_steps_per_second': 0.02}\n","\n","== Test Metrics ==\n","{'eval_loss': 1.203233003616333, 'eval_rouge1': 0.3793, 'eval_rouge2': 0.1493, 'eval_rougeL': 0.2591, 'eval_meteor': 0.3471, 'eval_runtime': 49.9964, 'eval_samples_per_second': 0.2, 'eval_steps_per_second': 0.02, 'epoch': 3.0}\n","{'eval_loss': 1.203233003616333, 'eval_meteor': 0.3471, 'eval_rouge1': 0.3793, 'eval_rouge2': 0.1493, 'eval_rougeL': 0.2591, 'eval_runtime': 49.9964, 'eval_samples_per_second': 0.2, 'eval_steps_per_second': 0.02}\n"]}]},{"cell_type":"code","source":["# pip install transformers datasets evaluate tqdm nltk\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import logging\n","from transformers import logging as hf_logging\n","import datasets, evaluate\n","\n","# ─── Silence library logs ────────────────────────────────────────────────────\n","hf_logging.set_verbosity_error()\n","datasets.logging.set_verbosity_error()\n","evaluate.logging.set_verbosity_error()\n","logging.getLogger(\"nltk\").setLevel(logging.ERROR)\n","logging.getLogger(\"transformers.generation_utils\").setLevel(logging.ERROR)\n","\n","import torch\n","import numpy as np\n","from datasets import load_dataset, DatasetDict\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSeq2SeqLM,\n","    DataCollatorForSeq2Seq,\n","    Seq2SeqTrainingArguments,\n","    Seq2SeqTrainer,\n",")\n","\n","# ─── CONFIG (customize here) ────────────────────────────────────────────────\n","MODEL_NAME         = \"facebook/bart-large-cnn\"\n","OUTPUT_DIR         = \"./bart_cnn_sum\"\n","BATCH_SIZE         = 16         # per device\n","NUM_EPOCHS         = 3\n","MAX_INPUT_LENGTH   = 512\n","MAX_OUTPUT_LENGTH  = 64\n","GEN_MAX_LENGTH     = MAX_OUTPUT_LENGTH * 2\n","\n","# Set to an int to debug on a subset, or None to use the full splits:\n","TRAIN_SIZE = 200\n","VAL_SIZE   = 15\n","TEST_SIZE  = 15\n","\n","# Logging / checkpoint intervals (in steps)\n","LOG_STEPS        = 50\n","EVAL_STEPS       = 200\n","SAVE_STEPS       = 200\n","SAVE_TOTAL_LIMIT = 3\n","\n","# ─── DEVICE ────────────────────────────────────────────────────────────────\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# ─── METRICS ───────────────────────────────────────────────────────────────\n","rouge  = evaluate.load(\"rouge\")\n","meteor = evaluate.load(\"meteor\")\n","\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","    dec_preds  = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    labels     = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    dec_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    r = rouge.compute(predictions=dec_preds, references=dec_labels, use_stemmer=True)\n","    m = meteor.compute(predictions=dec_preds, references=dec_labels)\n","    return {\n","        \"rouge1\": round(r[\"rouge1\"],4),\n","        \"rouge2\": round(r[\"rouge2\"],4),\n","        \"rougeL\": round(r[\"rougeL\"],4),\n","        \"meteor\": round(m[\"meteor\"],4),\n","    }\n","\n","# ─── LOAD & SAMPLE ─────────────────────────────────────────────────────────\n","print(\"Loading CNN/DailyMail 3.0.0…\")\n","raw = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n","\n","def sample_split(split, size):\n","    ds = raw[split]\n","    return ds.shuffle(seed=42).select(range(size)) if size is not None else ds\n","\n","data = DatasetDict({\n","    \"train\":      sample_split(\"train\",      TRAIN_SIZE),\n","    \"validation\": sample_split(\"validation\", VAL_SIZE),\n","    \"test\":       sample_split(\"test\",       TEST_SIZE),\n","})\n","print(f\"  → train={len(data['train'])}, val={len(data['validation'])}, test={len(data['test'])}\")\n","\n","# ─── TOKENIZER & PREPROCESS ─────────────────────────────────────────────────\n","print(\"Tokenizing…\")\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","\n","def preprocess(batch):\n","    inputs = tokenizer(\n","        batch[\"article\"],\n","        max_length=MAX_INPUT_LENGTH,\n","        truncation=True,\n","        padding=\"max_length\"\n","    )\n","    labels = tokenizer(\n","        batch[\"highlights\"],\n","        max_length=MAX_OUTPUT_LENGTH,\n","        truncation=True,\n","        padding=\"max_length\"\n","    ).input_ids\n","    inputs[\"labels\"] = labels\n","    return inputs\n","\n","tokenized = data.map(\n","    preprocess,\n","    batched=True,\n","    num_proc=3,\n","    remove_columns=[\"article\",\"highlights\"]\n",")\n","\n","# ─── MODEL & COLLATOR ───────────────────────────────────────────────────────\n","print(f\"Loading model to {device}…\")\n","model    = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\n","collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n","\n","# ─── TRAINING ARGS ──────────────────────────────────────────────────────────\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir                 = OUTPUT_DIR,\n","    num_train_epochs           = NUM_EPOCHS,\n","    per_device_train_batch_size= BATCH_SIZE,\n","    per_device_eval_batch_size = BATCH_SIZE,\n","\n","    logging_strategy           = \"steps\",\n","    logging_steps              = LOG_STEPS,\n","    logging_first_step         = True,\n","\n","    evaluation_strategy        = \"steps\",\n","    eval_steps                 = EVAL_STEPS,\n","\n","    save_strategy              = \"steps\",\n","    save_steps                 = SAVE_STEPS,\n","    save_total_limit           = SAVE_TOTAL_LIMIT,\n","\n","    disable_tqdm               = True,\n","    predict_with_generate      = True,\n","    fp16                       = (device.type==\"cuda\"),\n","    load_best_model_at_end     = True,\n","    metric_for_best_model      = \"rougeL\",\n","    greater_is_better          = True,\n","    report_to                  = \"none\",\n",")\n","\n","# ─── TRAINER ────────────────────────────────────────────────────────────────\n","trainer = Seq2SeqTrainer(\n","    model           = model,\n","    args            = training_args,\n","    train_dataset   = tokenized[\"train\"],\n","    eval_dataset    = tokenized[\"validation\"],\n","    tokenizer       = tokenizer,\n","    data_collator   = collator,\n","    compute_metrics = compute_metrics,\n",")\n","\n","# ─── TRAIN & RESUME ─────────────────────────────────────────────────────────\n","print(\" Fine-tuning started…\")\n","trainer.train()  # resume later with resume_from_checkpoint=True\n","\n","# ─── FINAL EVALUATION ───────────────────────────────────────────────────────\n","print(\"\\n== Validation Metrics ==\")\n","val_m = trainer.evaluate(tokenized[\"validation\"])\n","print({k: val_m[k] for k in sorted(val_m) if k.startswith(\"eval_\")})\n","\n","print(\"\\n== Test Metrics ==\")\n","test_m = trainer.evaluate(tokenized[\"test\"])\n","print({k: test_m[k] for k in sorted(test_m) if k.startswith(\"eval_\")})\n","\n","# ─── EXAMPLE GENERATIONS ────────────────────────────────────────────────────\n","print(\"\\n== Example generations on test set ==\")\n","for idx in range(min(5, len(data[\"test\"]))):\n","    sample = data[\"test\"][idx]\n","    print(f\"\\n--- SAMPLE {idx+1} ---\")\n","    print(\"\\nARTICLE:\")\n","    print(sample[\"article\"])\n","    inputs = tokenizer(\n","        sample[\"article\"],\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        padding=\"longest\",\n","        max_length=MAX_INPUT_LENGTH\n","    ).to(device)\n","    outputs = model.generate(\n","        **inputs,\n","        max_length=GEN_MAX_LENGTH,\n","        num_beams=4,\n","        early_stopping=True\n","    )\n","    print(\"\\nGENERATED SUMMARY:\")\n","    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n","    print(\"\\nREFERENCE HIGHLIGHTS:\")\n","    print(sample[\"highlights\"])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["9c589499e6564b7e813d9af99e209141","144983a9738b4b1095886d26a5112f84","512dac534ae14a1eb0d97352dc8e04dc","44b97d4f1b4845229dab8eed92616a44","cae1cf2b3a764e26aef92a1f5642310e","e995d0a017e440afa0bba1aafba05814","3b584dfb501d431aafe27a174280c718","d1bb9a961d874b74bb2f078881c23988","beb082069bc64ea5bf86d4966076803f","6d7628191b5041a88d7e31bec81b9010","85acb66889974a618320a571f4287ddb","799a63d56a554d6aafd64c7aed534c7f","a31c8bad3fc1403fb8b69bd764217b30","945841ba4235485691d4d089d5352486","212f90fdfe964a8ca76e4d6046e9d731","925dcbd4437649b39403eccd34f4a4fd","8ba3943a4add49a29d466c6fc0e6284a","ea245f73ebbf4600b30bb015d0f82f7b","0b5c3adc7c37480e805701344dca41d3","24a6c2da4c934cdf86fb1846ae080d38","b8dcaee53b9d471ab0234f756845115c","3ce05e49f6c44aae99cd0b9df275c7ce","b12cabf6f3e84f8999bccbe8b5b2cc89","26d5c3bf35274f75b1b8bc6bf5eac823","47fba816b7a74207a15a1fb8d8ba246d","04d906fbc65c4a32a7704e50264f0897","025435c044744fa8bcf806bd6cca1148","a38208d478324e04a55bb2bd2247f601","800b6e86437a451795802a5bd7f68ab6","c846292126e641bcaf8fe584e12ce97c","6aa6e39f7bc84d54a6a65bdc85b49469","4fec8467f3824a2990d7b731fa4ba229","d34c5ebfd90640cf9976993556eb6916"]},"id":"MGygY7d2XisM","executionInfo":{"status":"ok","timestamp":1746400094561,"user_tz":300,"elapsed":2316914,"user":{"displayName":"Akhil Mavallapalli","userId":"02508402536572482827"}},"outputId":"930c3f99-28da-4b8b-8212-512e75515263"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Loading CNN/DailyMail 3.0.0…\n","  → train=200, val=15, test=15\n","Tokenizing…\n"]},{"output_type":"display_data","data":{"text/plain":["Map (num_proc=3):   0%|          | 0/200 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c589499e6564b7e813d9af99e209141"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map (num_proc=3):   0%|          | 0/15 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"799a63d56a554d6aafd64c7aed534c7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map (num_proc=3):   0%|          | 0/15 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b12cabf6f3e84f8999bccbe8b5b2cc89"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loading model to cpu…\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id. If id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running training *****\n","  Num examples = 200\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 39\n","  Number of trainable parameters = 406290432\n","You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"stream","name":"stdout","text":[" Fine-tuning started…\n","{'loss': 3.2265, 'learning_rate': 4.871794871794872e-05, 'epoch': 0.08}\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id. If id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 15\n","  Batch size = 16\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"min_length\": 56,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["{'train_runtime': 2099.4724, 'train_samples_per_second': 0.286, 'train_steps_per_second': 0.019, 'train_loss': 1.4119522754962628, 'epoch': 3.0}\n","\n","== Validation Metrics ==\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id. If id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 15\n","  Batch size = 16\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"min_length\": 56,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.8956702947616577, 'eval_rouge1': 0.4363, 'eval_rouge2': 0.2098, 'eval_rougeL': 0.3005, 'eval_meteor': 0.403, 'eval_runtime': 75.0714, 'eval_samples_per_second': 0.2, 'eval_steps_per_second': 0.013, 'epoch': 3.0}\n","{'eval_loss': 1.8956702947616577, 'eval_meteor': 0.403, 'eval_rouge1': 0.4363, 'eval_rouge2': 0.2098, 'eval_rougeL': 0.3005, 'eval_runtime': 75.0714, 'eval_samples_per_second': 0.2, 'eval_steps_per_second': 0.013}\n","\n","== Test Metrics ==\n"]},{"output_type":"stream","name":"stderr","text":["Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"min_length\": 56,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.6750235557556152, 'eval_rouge1': 0.4049, 'eval_rouge2': 0.1778, 'eval_rougeL': 0.2901, 'eval_meteor': 0.3941, 'eval_runtime': 79.9912, 'eval_samples_per_second': 0.188, 'eval_steps_per_second': 0.013, 'epoch': 3.0}\n","{'eval_loss': 1.6750235557556152, 'eval_meteor': 0.3941, 'eval_rouge1': 0.4049, 'eval_rouge2': 0.1778, 'eval_rougeL': 0.2901, 'eval_runtime': 79.9912, 'eval_samples_per_second': 0.188, 'eval_steps_per_second': 0.013}\n","\n","== Example generations on test set ==\n","\n","--- SAMPLE 1 ---\n","\n","ARTICLE:\n","(CNN) I see signs of a revolution everywhere. I see it in the op-ed pages of the newspapers, and on the state ballots in nearly half the country. I see it in politicians who once preferred to play it safe with this explosive issue but are now willing to stake their political futures on it. I see the revolution in the eyes of sterling scientists, previously reluctant to dip a toe into this heavily stigmatized world, who are diving in head first. I see it in the new surgeon general who cites data showing just how helpful it can be. I see a revolution in the attitudes of everyday Americans. For the first time a majority, 53%, favor its legalization, with 77% supporting it for medical purposes. Support for legalization has risen 11 points in the past few years alone. In 1969, the first time Pew asked the question about legalization, only 12% of the nation was in favor. I see a revolution that is burning white hot among young people, but also shows up among the parents and grandparents in my kids' school. A police officer I met in Michigan is part of the revolution, as are the editors of the medical journal, Neurosurgery. I see it in the faces of good parents, uprooting their lives to get medicine for their children -- and in the children themselves, such as Charlotte, who went from having 300 seizures a week to just one or two a month. We know it won't consistently have such dramatic results (or any impact at all) in others, but what medicine does? I see this medical marijuana revolution in surprising places. Girl's seizures spur medical marijuana legislation in Georgia . Among my colleagues, my patients and my friends. I have even seen the revolution in my own family. A few years ago, when I told my mother I was investigating the topic for a documentary, I was met with a long pause. \"Marijuana...?\" She whispered in a half questioning, half disapproving tone. She could barely even say the word and her response filled me with self-doubt. Even as a grown man, mom can still make my cheeks turn red and shatter my confidence with a single word. But just last week she suddenly stopped mid-conversation and said, \"I am proud of you on the whole marijuana thing.\" I waited for the other shoe to drop, but it didn't. Instead, she added, \"You probably helped a lot of people who were suffering.\" I don't think we had ever had a conversation like that one. At that moment, I saw a revolution that can bring you to tears. The word revolution, comes from the Latin revolutio, to \"turn around.\" I had my own turn around a couple of years ago, and at the time it was a lonely place to hold a supportive position on medical marijuana. Hardly any government officials would agree to sit down and be interviewed on the topic. Even patients I spoke to were reluctant to share their stories. It can be tricky, I learned, to be on the right side of science but on the wrong side of ideology. When we put the first \"Weed\" documentary on television in August 2013, I didn't know if anyone would watch our yearlong investigation. Even worse, I didn't even know if they would care. Is weed legal in your state? Just two years later, in \"Weed 3,\" we are eyewitnesses to a revolution in full swing. You will ride along with us for the dawn of the first federally approved clinical study on the use of marijuana for PTSD. You will meet patients such as Sean Kiernan, an accomplished investment banker, and Amelia Taylor, a stay-at-home mom. They are the remarkable and surprising faces of this revolution -- smart, successful and suffering -- unwilling to accept the fact that commonly prescribed medications often used to treat PTSD can be worse than the underlying disorder itself. Sean Kiernan nearly died, trying to get better. You will see what weed really does to your brain, in crystal clear images. This time around, you will hear from the heads of government agencies earnestly sharing their point of view, both Democratic and Republican senators, and even the President of the United States. This is what a revolution looks like. Your medical marijuana questions answered . When \"Weed 2: Cannabis Madness\" aired in March 2014, Boston researcher Rick Doblin believed the right people were watching. Just four days later, Doblin received a letter in the mail he had been waiting on for seven years that finally provided federal approval for his marijuana study. The federal farm where Doblin would have to obtain his marijuana is on the campus of Ole Miss in Oxford, Mississippi. In anticipation of a scientific revolution, the production of research-grade marijuana there has increased 30-fold in just the past year. Make no mistake, we have plenty of evidence that the approval and support of the federal government can fast track a revolution at a faster pace than we have yet seen. It was the National Institute of Allergy and Infectious Diseases that spearheaded the research into a cure for AIDS, as well as stopping the spread of West Nile Virus. They were also responsible for the awesome task of eradicating polio and smallpox. Other successful federally backed programs include the human genome project, the BRAIN initiative and the Precision Medicine Initiative. There are no shortage of examples where the federal government has been a guardian of our public health needs, and you could argue that medical marijuana would also qualify as a worthwhile investment. 10 diseases where medical marijuana could have impact . There is now promising research into the use of marijuana that could impact tens of thousands of children and adults, including treatment for cancer, epilepsy and Alzheimer's, to name a few. With regard to pain alone, marijuana could greatly reduce the demand for narcotics and simultaneously decrease the number of accidental painkiller overdoses, which are the greatest cause of preventable death in this country. As I sat across from Sens. Kirsten Gillibrand (D-New York) and Cory Booker (D-New Jersey), I knew something extraordinary was happening. They were reciting the story of Charlotte Figi and countless other children. They were quoting back the data we had shared from our earlier investigations. They were extolling the potential virtues of the plant, and all of that was before the interview even started. There was an impatience about them, and they seemed in a hurry to make a large dent in marijuana reform. They want marijuana to be rescheduled. They want it now. They want doctors to be able to prescribe it at VA hospitals all over the country. They want it now. They want research dollars freed up to study the plant. They want it now. They want their fellow lawmakers at the state and national level to acknowledge what most of the world, including the citizens of the United States, have known for a long time: Marijuana is a medicine, that should be studied and treated like any other medicine. And they want all of it now. I spent much of our interview challenging them. I needed to remind them that people, long before me or them, have been trying to do many of these same things for 40 years, and had been rejected every time. I reminded them that politicians have a hard time winning elections on the issue of marijuana but less difficulty losing them. I challenged them every step of the way. \"This time will be different,\" Booker confidently told me as he walked out of the room. Is marijuana as safe as -- or safer than -- alcohol? I know how easy it is do nothing because I did nothing for too long. Take a good look at the data, educate yourself and talk to the patients, who are often out of options and find their hope in the form of a simple plant. Journalists shouldn't take a position. It makes sense. Objectivity is king. But, at some point, open questions do get answered. At some point, contentious issues do get resolved. At some point, common sense prevails. So, here it is: We should legalize medical marijuana. We should do it nationally. And, we should do it now. 9 things to know about legal pot .\n"]},{"output_type":"stream","name":"stderr","text":["Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"min_length\": 56,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["\n","GENERATED SUMMARY:\n","CNN's John Sutter says he sees signs of a revolution in attitudes toward medical marijuana.\n","For the first time, 53% of Americans favor its legalization, with 77% supporting it for medical purposes.\n","Sutter: Support for legalization has risen 11 points in the past few years alone.\n","He says the issue is burning white hot among young people, but also shows up among parents.\n","\n","REFERENCE HIGHLIGHTS:\n","CNN's Dr. Sanjay Gupta says we should legalize medical marijuana now .\n","He says he knows how easy it is do nothing \"because I did nothing for too long\"\n","\n","--- SAMPLE 2 ---\n","\n","ARTICLE:\n","He looks barely teenage. But this child has amassed thousands of Twitter followers with his pictorial updates of 'gang life'. The baby-faced boy from Memphis, Tennessee, poses with guns, cash, and bags of what looks like marijuana. Scroll down for video . Baby-faced: This little boy has amassed more than 3,000 followers on Twitter with pictures like these . In many pictures he is smoking suspicious substances, with captions such as 'High Life' Backlash: The boy, from Memphis, has prompted a wave of critics calling his stunts 'sad' In one video he laughs and points the gun at the camera in an apparent attempt to look menacing - as adults laugh in the background. In others, he is pictured blowing smoke, with the caption: 'High Life'. Tweets include the phrases, 'I need a bad b****', 'f*** da police', and 'gang sh** n****'. Some feature sexual references, such as: 'quit suckin my d***'. As he is a minor, DailyMail.com will not identify the little boy. The child's tweets have prompted backlash from other Twitter users and members of the community. One woman tweeted him: 'you need help i feel so bad for you your parents should be ashamed please get help and a education this is sad and disappointing.' Another Twitter user said: 'You poor baby. Just stop it & go to school.' Adults around: Many of his pictures and videos are taken with adults who are seen laughing, unfazed . Guns: The little boy, who appears to be barely teenage, regularly poses for his followers with guns . The boy hit back at his critics claiming he has had a tough year and the backlash is 'stressing out' his mother . Orrden Williams, a Memphis resident who has been the victim of gang violence, told AOL.com: 'What he's doing on here is something that he's going to bring upon society one day. It's just inevitable.' He added: 'All it takes is the right influence, and all this can change. So that's a wake-up Memphis call right there.' Hitting back at critics, the boy tweeted on Friday: 'Wish some people would mind dey own business' as 'it's stressing my mama out'. He adds that his sister has 'been locked up for two years' and his critics 'don't know the half of what we been through end of 2014 & beg of 2015.' Authorities are increasingly cognizant of Twitter as a means of perpetuating gang culture. While many like this young boy use it as a forum to flaunt their drugs and guns, around 50 per cent of organized gangs maintain in the United States maintain a social media profile.\n"]},{"output_type":"stream","name":"stderr","text":["Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"min_length\": 56,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["\n","GENERATED SUMMARY:\n","The baby-faced boy from Memphis, Tennessee, regularly posts pictures of 'gang life' on Twitter with captions like: 'I need a bad b****', 'f*** da police', and 'gang sh** n****'\n","In one video he laughs and points a gun at the camera in an apparent attempt to look menacing - as adults laugh in the background.\n","As a minor, DailyMail.com will not identify the boy, who has more than 3,000 followers.\n","He has prompted a wave of critics calling his stunts'sad' and'stressing out' his mother.\n","\n","REFERENCE HIGHLIGHTS:\n","Child has amassed thousands of Twitter followers with 'gang life' photos .\n","In one video he points gun at camera as adults look on unfazed .\n","His tweets have prompted backlash with calls for intervention .\n","\n","--- SAMPLE 3 ---\n","\n","ARTICLE:\n","New Jersey Governor Chris Christie wasn't looking too presidential Tuesday night when he got into a heated debate with a veteran teacher at a town hall meeting. And now the state's largest teacher's union is calling him out for his 'bullying' behavior. 'He's always taken a very nasty and disrespectful tone with teachers and other individuals who dare to question him at these events,' Steve Wollmer of the NJ Education Association told NJ.com. 'It's the one thing that never seems to change.' Scroll Down for Video . Not being nice: New Jersey Gov Chris Christie (left) is being called a bully for the way he interacted with a teacher (Kathy Mooney, right) at a Tuesday night town hall meeting . That sentiment doesn't ring well with Christie's ambitions to run in the Republican presidential primaries next year. Tuesday night, Christie appeared at Kenilworth Town Hall to take questions from a group of citizens, when Kathy Mooney, a high school English Teacher from Roselle Park, took the microphone. Ms Mooney, who has been a teacher for 27 years, questioned Christie's motivations behind a legal settlement with oil company ExxonMobil which could have contributed drastically to the state's pension plans for teachers. Christie settled the deal for $225million, despite the fact that the state had originally asked for $8.9billion which Mooney described as 'favoring the affluent' and 'kicking state workers under the bus'. 'I know that you could have gotten more money, on the dollar,' Mooney said. 'Do you?' a defensive Christie quickly responded. 'You do know that? Really? You know that?' Mooney started to respond, but not before being cut off by the governor. A good deal? Mooney questioned Christie's decision-making behind a $225million legal settlement. The state originally wanted $8.9billion from the oil company, and Mooney says that would have had a huge impact on pensions for public employees . 'I mean: Do you know that?' Christie continued. 'I wanna know how you know that. Because you're a teacher, and you're standing in front of students every day, conveying to them, facts - things that they need to learn. So I would like to understand your analysis of how you know that in a ten year long, court case, that you have enough detail to know.' Mooney explains that she read about the deal in the newspaper and did not attend the meeting 'to be bullied'. 'You're not being bullied, because you're asking me a question, I'm going to ask you questions back,' Christie says, as he continues to dodge the issue. 'He said, \"I'm not bullying you' as he bullied her,\"' Wollmer said of the exchange. Perhaps the reason why Wollmer and his union responded sharply to Christie's town hall meeting Tuesday night, is that he blamed the union for their role in the current pension system. 'The fact is your union, over the course of time, has asked for significantly higher benefits - more expensive benefits - that your union knew the state could not afford,' Christie said Tuesday.\n"]},{"output_type":"stream","name":"stderr","text":["Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"min_length\": 56,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["\n","GENERATED SUMMARY:\n","New Jersey Gov Chris Christie got into a heated debate with a veteran English teacher at a town hall meeting Tuesday night.\n","Kathy Mooney questioned Christie's motivations behind a $225million legal settlement with oil company ExxonMobil.\n","The state's largest teacher's union is calling him out for his 'bullying' behavior.\n","Steve Wollmer of the NJ Education Association said: 'He's always taken a very nasty and disrespectful tone with teachers'\n","\n","REFERENCE HIGHLIGHTS:\n","The presidential hopeful held a town hall meeting in Kenilworth on Tuesday .\n","During the meeting, high school English teacher Kathy Mooney got up to ask the governor a question about pensions .\n","She asked why he didn't seek a higher legal settlement in a case with ExxonMobil that would have contributed to the state's pension system .\n","Christie responded by repeatedly asking how much Mooney knew about the deal instead of answering her question .\n","\n","--- SAMPLE 4 ---\n","\n","ARTICLE:\n","YouTube star Cassey Ho has hit back at critics with a powerful and provocative new video, highlighting the cruel comments left by viewers of her fitness-focused clips who accuse the trim and toned online icon of being everything from ‘too fat’ to ‘ugly’ to ‘pudgy’. Cassey's YouTube channel, Blogilates, has over two million subscribers, so fans may have been surprised to learn that the negative comments left under the California resident’s fitness videos still manage to get under her skin – and have a much greater impact on her than any of the positive messages sent to her by her fans. But though mean and critical messages may have put a dent in the 28-year-old’s self-esteem, the workout pro is getting the last word with her latest video, The 'Perfect' Body, which examines just whether conforming to society's standards is the key to happiness, while highlighting some of the cruel comments she has received from viewers of her YouTube videos. Scroll down for video. Fighting back: In her new video, Pilates instructor and YouTube star Cassey Ho combats body-shamers who comment on her videos . She's only human: Seemingly full of confidence, Cassey said even she has felt bad about herself after reading nasty comments calling her 'fat' online . In a post on her Blogilates blog, Cassey said negative comments on her videos are nothing new, but the flood of nastiness has grown especially bad lately. She added that the trash-talking has even brought her to tears on more than one occasion. 'It’s hard to be content with the shape of your body when people are constantly telling you how fat you are, how much weight you need to lose, how much weight you need to gain,' she wrote. 'What do people want?' Cassey goes on to explain that, through her new video, fans can see what it's like to face that kind of negativity, and watch as her own self-esteem gets 'stripped away', leaving her feeling insecure and ashamed about her appearance. Fit or fat: The slim and toned blogger says that the negativity in the comments section of her YouTube channel has become especially nasty lately, and has even made her cry . Peanut gallery problems: In the new video, Cassey picks at her body, honing in on places that commenters say she could improve upon . The video begins with Cassey posting a selfie in gym clothes, taken just after a workout. She immediately starts to read the comments, and though they start off positive, soon people are telling her she shouldn't give fitness advice because she is fat. Cassey goes on to strip down to her underwear, examining, pinching and prodding her body, using the spiteful comments to direct her to different parts of her physique that people insist need changing. And as the video goes on, she grows more unsatisfied with herself as negative comments pop up around her. So the YouTube star gets to work, manipulating her body in a simulation of real-life Photoshop. She thins her thighs, hips, and waist, makes her butt and chest bigger, and even changes the color of her eyes. Finally, she posts another selfie of her 'improved' look - achieved with the help of a body double - but she is clearly still unhappy, and she uses this feeling to try and show viewers that giving in to body-shaming will only leave you worse off. Before and after: The fitness expert's body goes under a major transformation; Cassey used a body double to achieve the final look . Moral message: Even after making her butt and chest bigger and her waist and legs smaller, Cassey still doesn't find happiness at the end of the video . No Photoshop here: The retouching in Cassey's video was meant to prove a point - she doesn't use Photoshop in the photos she posts online . It seems that her 'take that' attitude has left her detractors speechless. Readers of her blog flooded the comments section of the latest post with overwhelmingly positive feedback. 'Thank you for standing up to this struggle for girls and women everywhere,' wrote one fan. Another chimed in: 'I absolutely applaud that you are your natural, beautiful self. I think you’re a wonderful inspiration for young people.' Cassey hopes viewers who relate to the video's message will share it in an effort to battle body shaming and cyber bullying.\n"]},{"output_type":"stream","name":"stderr","text":["Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"min_length\": 56,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["\n","GENERATED SUMMARY:\n","The 28-year-old fitness blogger has released a new video in response to the barrage of negative comments she has received from viewers of her fitness-focused YouTube videos.\n","In the video, she explores whether conforming to society's standards is the key to happiness, while highlighting some of the cruel comments she's received online.\n","She says the trash-talking has brought her to tears on more than one occasion, and has made her feel self-conscious about her appearance.\n","\n","REFERENCE HIGHLIGHTS:\n","Cassey Ho boasts over two million subscribers on her YouTube channel Blogilates .\n","The 28-year-old receives hundreds of comments a day telling her that she needs to lose weight .\n","\n","--- SAMPLE 5 ---\n","\n","ARTICLE:\n","British taekwondo fighter Aaron Cook has confirmed he plans to compete for Moldova at the 2016 Olympics in Rio. Dorset-born Cook, 24, who was overlooked for the Great Britain taekwondo squad at London 2012, applied for citizenship after receiving funding from Moldovan billionaire Igor Iuzefovici and has now received his passport from the small eastern European state. 'I will soon begin a new journey representing the Republic of Moldova at all International competitions and hopefully the Rio Olympic games and beyond, should I qualify,' Cook wrote on his Facebook page. Three time European champion Aaron Cook (right) has refused to fight for Great Britain  since May 2012 . The British taekwondo star has been granted citizenship by Moldova and plans to fight for them in Rio 2016 . 'Although I am upset and disappointed I will not represent my country of birth at another major championships, I felt I had no other option. 'I am a fighter at heart and I am not going to throw away 20 years of dedication because of bureaucracy.' Cook, who fought for Team GB at Beijing 2008, felt he was overlooked for London 2012 - when Lutalo Muhammad, who was selected instead, won bronze - because he decided to quit the British programme in 2011. Cook is aggrieved at being overlooked in favour of Lutalo Muhammad, who won bronze at London 2012 . GB Taekwondo has always denied this, but Cook has refused to compete under the British flag since May 2012 and has fought for the Isle of Man since early 2013. 'It has been an amazing couple of years full of memories, and I would have loved to continue competing under the Isle of Man banner,' Cook said. 'Unfortunately that was never going to be possible as it was made clear to me in May of last year, after winning the European Championships for the third consecutive time, that it would not be possible for me to be selected for Team GB, regardless of my world ranking or performances. Cook competes against Mauro Sarmiento at the Olympic Games in Beijing in 2008 . 'Having received no funding or support from the GB system and financing myself since June 2011, this was not a situation that was acceptable to me and I did not want to put myself, family, friends, supporters or sport through the same situation we were forced to endure at the London 2012 Olympic Games.' Cook hopes to represent Moldova for the first time at the inaugural Baku European Games in June having served the mandatory three-year period since representing a previous country. However, the British Olympic Association could yet block the move and Sportsmail understands it is still in talks over the matter.\n","\n","GENERATED SUMMARY:\n","Aaron Cook was overlooked for the Great Britain taekwondo squad at London 2012.\n","The 24-year-old has refused to fight for Britain since May 2012 and has fought for the Isle of Man since.\n","Moldova billionaire Igor Iuzefovici funded Cook's application for citizenship.\n","He has now received his passport and plans to represent Moldova at Rio 2016.\n","Cook won three European titles at European Championships in 2012 and 2013.\n","\n","REFERENCE HIGHLIGHTS:\n","Aaron Cook was overlooked by Team GB for the London Olympics .\n","Taekwondo star has received citizenship from Moldova and plans to fight for them at the Rio 2016 Games .\n","The British Olympic Association could yet block the move .\n"]}]},{"cell_type":"code","source":["# pip install transformers datasets evaluate tqdm nltk\n","\n","import warnings, logging, contextlib, io\n","warnings.filterwarnings(\"ignore\")\n","\n","from transformers import logging as hf_logging\n","import datasets, evaluate\n","\n","# ─── Silence library logs ───────────────────────────────────────────────────\n","hf_logging.set_verbosity_error()\n","datasets.logging.set_verbosity_error()\n","evaluate.logging.set_verbosity_error()\n","logging.getLogger(\"nltk\").setLevel(logging.ERROR)\n","\n","import torch, numpy as np\n","from datasets import load_dataset, DatasetDict\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSeq2SeqLM,\n","    DataCollatorForSeq2Seq,\n","    Seq2SeqTrainingArguments,\n","    Seq2SeqTrainer,\n",")\n","\n","# ─── CONFIG ────────────────────────────────────────────────────────────────\n","MODEL_NAME         = \"facebook/bart-large-cnn\"\n","OUTPUT_DIR         = \"./bart_cnn_sum\"\n","BATCH_SIZE         = 8\n","NUM_EPOCHS         = 3\n","MAX_INPUT_LENGTH   = 512\n","MAX_OUTPUT_LENGTH  = 142\n","GEN_MAX_LENGTH     = MAX_OUTPUT_LENGTH * 2\n","\n","TRAIN_SIZE = 20    # or None for full\n","VAL_SIZE   = 3\n","TEST_SIZE  = 3\n","\n","LOG_STEPS        = 50\n","EVAL_STEPS       = 200\n","SAVE_STEPS       = 200\n","SAVE_TOTAL_LIMIT = 3\n","\n","# ─── DEVICE ────────────────────────────────────────────────────────────────\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# ─── METRICS ───────────────────────────────────────────────────────────────\n","rouge  = evaluate.load(\"rouge\")\n","meteor = evaluate.load(\"meteor\")\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    if isinstance(preds, tuple): preds = preds[0]\n","    dec_preds  = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    labels     = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    dec_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    r = rouge.compute(predictions=dec_preds, references=dec_labels, use_stemmer=True)\n","    m = meteor.compute(predictions=dec_preds, references=dec_labels)\n","    return {\n","        \"rouge1\": round(r[\"rouge1\"],4),\n","        \"rouge2\": round(r[\"rouge2\"],4),\n","        \"rougeL\": round(r[\"rougeL\"],4),\n","        \"meteor\": round(m[\"meteor\"],4),\n","    }\n","\n","# ─── LOAD & SAMPLE ─────────────────────────────────────────────────────────\n","print(\"Loading CNN/DailyMail 3.0.0…\")\n","raw = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n","def sample_split(name, n):\n","    ds = raw[name]\n","    return ds.shuffle(seed=42).select(range(n)) if n is not None else ds\n","\n","data = DatasetDict({\n","    \"train\":      sample_split(\"train\",      TRAIN_SIZE),\n","    \"validation\": sample_split(\"validation\", VAL_SIZE),\n","    \"test\":       sample_split(\"test\",       TEST_SIZE),\n","})\n","print(f\"  → train={len(data['train'])}, val={len(data['validation'])}, test={len(data['test'])}\")\n","\n","# ─── PREPROCESS ─────────────────────────────────────────────────────────────\n","print(\"Tokenizing…\")\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","def preprocess(batch):\n","    inp = tokenizer(batch[\"article\"],\n","        max_length=MAX_INPUT_LENGTH, truncation=True, padding=\"max_length\")\n","    lbl = tokenizer(batch[\"highlights\"],\n","        max_length=MAX_OUTPUT_LENGTH, truncation=True, padding=\"max_length\").input_ids\n","    inp[\"labels\"] = lbl\n","    return inp\n","\n","tokenized = data.map(\n","    preprocess,\n","    batched=True,\n","    num_proc=4,\n","    remove_columns=[\"article\",\"highlights\"]\n",")\n","\n","# ─── MODEL & TRAINER ───────────────────────────────────────────────────────\n","print(f\"Loading model onto {device}…\")\n","model    = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\n","collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir                 = OUTPUT_DIR,\n","    num_train_epochs           = NUM_EPOCHS,\n","    per_device_train_batch_size= BATCH_SIZE,\n","    per_device_eval_batch_size = BATCH_SIZE,\n","\n","    logging_strategy   =\"steps\",\n","    logging_steps      = LOG_STEPS,\n","    logging_first_step = True,\n","\n","    evaluation_strategy=\"steps\",\n","    eval_steps         = EVAL_STEPS,\n","\n","    save_strategy      =\"steps\",\n","    save_steps         = SAVE_STEPS,\n","    save_total_limit   = SAVE_TOTAL_LIMIT,\n","\n","    disable_tqdm       = True,\n","    predict_with_generate=True,\n","    fp16               = (device.type==\"cuda\"),\n","    load_best_model_at_end= True,\n","    metric_for_best_model =\"rougeL\",\n","    greater_is_better     = True,\n","    report_to             =\"none\",\n",")\n","\n","trainer = Seq2SeqTrainer(\n","    model           = model,\n","    args            = training_args,\n","    train_dataset   = tokenized[\"train\"],\n","    eval_dataset    = tokenized[\"validation\"],\n","    tokenizer       = tokenizer,\n","    data_collator   = collator,\n","    compute_metrics = compute_metrics,\n",")\n","\n","# ─── TRAIN & EVAL ──────────────────────────────────────────────────────────\n","print(\"▶ Fine-tuning started…\")\n","trainer.train()\n","\n","print(\"\\n== Validation Metrics ==\")\n","val_m = trainer.evaluate(tokenized[\"validation\"])\n","print({k: val_m[k] for k in sorted(val_m) if k.startswith(\"eval_\")})\n","\n","print(\"\\n== Test Metrics ==\")\n","test_m = trainer.evaluate(tokenized[\"test\"])\n","print({k: test_m[k] for k in sorted(test_m) if k.startswith(\"eval_\")})\n","\n","# ─── EXAMPLE GENERATIONS ───────────────────────────────────────────────────\n","print(\"\\n== Example generations ==\")\n","for i in range(min(5, len(data[\"test\"]))):\n","    sample = data[\"test\"][i]\n","    print(f\"\\n--- SAMPLE {i+1} ---\")\n","    print(\"\\nARTICLE:\")\n","    print(sample[\"article\"])\n","    inputs = tokenizer(\n","        sample[\"article\"],\n","        max_length=MAX_INPUT_LENGTH,\n","        truncation=True,\n","        padding=\"longest\",\n","        return_tensors=\"pt\"\n","    ).to(device)\n","\n","    # suppress the GenerationConfig print\n","    with contextlib.redirect_stdout(io.StringIO()):\n","        outputs = model.generate(\n","            **inputs,\n","            max_length=GEN_MAX_LENGTH,\n","            num_beams=4,\n","            early_stopping=True\n","        )\n","\n","    print(\"\\nGENERATED SUMMARY:\")\n","    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n","    print(\"\\nREFERENCE HIGHLIGHTS:\")\n","    print(sample[\"highlights\"])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["42da0446dd694bb49363ef9b96b3417c","e52108467fd3427d9359323e5d44caed","f45f8a818c5b4efa82683b5d3f083245","aaac1d25fd6b40439eb21a4699f9f9d0","38acaa3a679046e8b63edef454d2fd25","3bcc48b7cc8e46b182523e37c1b115c1","e7e2a4a2a7614244a0965bc09c5f0c1d","0d75a68736f54237a28db7514e2c26b3","7b0d03c3608e4b49a1988dbbc5c72dfe","7857815fb0fa4e95947ddbcbc1e715bb","5ac56f1886ba4423a3be95554f045d1a","50f61df0854948cba5d35e1c250c6855","3e66c871d74e487990a8041a29f9ffc0","90759a93e53d464ab23a443f88de7425","2de28ce9cb27484ebca08f696d4c912d","27a81fe9feae498db16faf0666deba95","d8ca791115a64be59cefc07cf881e8c3","bb0fab66fd7e49c5ada1a84d0b6679e4","cc088e21809b436fa42ee52e474bd0a1","5909ab68d5ab4152b6ec7ec38f66147b","d563f474ac404a58ad43a37b645fe1a1","4bbbea0a1c244f96ac3d27cfeb176e29","ac03f53a67f74edd90a52c9fab6df6f7","5219da212261437ea8d02b87e6c3f2db","e0935c6b38d54f3d86cb72fa4fa98ed1","2c19ac68bbe44826ad683fb896940b26","fd16e2f150314274a0910cbd6bc43e29","f1164a9a8b9c4780869f6a6d1387f782","7b9f3c72c9994db580dc3d2139b4f86d","31bb4709c7874555974540a6cf1ca4a8","ddec86b0ee414930ac2e20f5a3a6510c","c55170974093467e97d4481fb634e2da","23123636ffce497abef1cdf37322af7b"]},"id":"5An4Y9iokmr-","executionInfo":{"status":"ok","timestamp":1746400720371,"user_tz":300,"elapsed":341181,"user":{"displayName":"Akhil Mavallapalli","userId":"02508402536572482827"}},"outputId":"af49f2cf-5d59-4a3f-e6bd-c798894eecd7"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Loading CNN/DailyMail 3.0.0…\n","  → train=20, val=3, test=3\n","Tokenizing…\n"]},{"output_type":"display_data","data":{"text/plain":["Map (num_proc=4):   0%|          | 0/20 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42da0446dd694bb49363ef9b96b3417c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50f61df0854948cba5d35e1c250c6855"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac03f53a67f74edd90a52c9fab6df6f7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loading model onto cpu…\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id. If id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running training *****\n","  Num examples = 20\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 9\n","  Number of trainable parameters = 406290432\n","You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"stream","name":"stdout","text":["▶ Fine-tuning started…\n","{'loss': 7.7924, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id. If id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 3\n","  Batch size = 8\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"min_length\": 56,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["{'train_runtime': 234.4663, 'train_samples_per_second': 0.256, 'train_steps_per_second': 0.038, 'train_loss': 4.841492811838786, 'epoch': 3.0}\n","\n","== Validation Metrics ==\n"]},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id. If id are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 3\n","  Batch size = 8\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"min_length\": 56,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 2.9673423767089844, 'eval_rouge1': 0.4388, 'eval_rouge2': 0.2191, 'eval_rougeL': 0.3598, 'eval_meteor': 0.4504, 'eval_runtime': 19.5329, 'eval_samples_per_second': 0.154, 'eval_steps_per_second': 0.051, 'epoch': 3.0}\n","{'eval_loss': 2.9673423767089844, 'eval_meteor': 0.4504, 'eval_rouge1': 0.4388, 'eval_rouge2': 0.2191, 'eval_rougeL': 0.3598, 'eval_runtime': 19.5329, 'eval_samples_per_second': 0.154, 'eval_steps_per_second': 0.051}\n","\n","== Test Metrics ==\n"]},{"output_type":"stream","name":"stderr","text":["Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"min_length\": 56,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 3.1277267932891846, 'eval_rouge1': 0.2882, 'eval_rouge2': 0.0607, 'eval_rougeL': 0.1626, 'eval_meteor': 0.259, 'eval_runtime': 23.8858, 'eval_samples_per_second': 0.126, 'eval_steps_per_second': 0.042, 'epoch': 3.0}\n","{'eval_loss': 3.1277267932891846, 'eval_meteor': 0.259, 'eval_rouge1': 0.2882, 'eval_rouge2': 0.0607, 'eval_rougeL': 0.1626, 'eval_runtime': 23.8858, 'eval_samples_per_second': 0.126, 'eval_steps_per_second': 0.042}\n","\n","== Example generations ==\n","\n","--- SAMPLE 1 ---\n","\n","ARTICLE:\n","(CNN) I see signs of a revolution everywhere. I see it in the op-ed pages of the newspapers, and on the state ballots in nearly half the country. I see it in politicians who once preferred to play it safe with this explosive issue but are now willing to stake their political futures on it. I see the revolution in the eyes of sterling scientists, previously reluctant to dip a toe into this heavily stigmatized world, who are diving in head first. I see it in the new surgeon general who cites data showing just how helpful it can be. I see a revolution in the attitudes of everyday Americans. For the first time a majority, 53%, favor its legalization, with 77% supporting it for medical purposes. Support for legalization has risen 11 points in the past few years alone. In 1969, the first time Pew asked the question about legalization, only 12% of the nation was in favor. I see a revolution that is burning white hot among young people, but also shows up among the parents and grandparents in my kids' school. A police officer I met in Michigan is part of the revolution, as are the editors of the medical journal, Neurosurgery. I see it in the faces of good parents, uprooting their lives to get medicine for their children -- and in the children themselves, such as Charlotte, who went from having 300 seizures a week to just one or two a month. We know it won't consistently have such dramatic results (or any impact at all) in others, but what medicine does? I see this medical marijuana revolution in surprising places. Girl's seizures spur medical marijuana legislation in Georgia . Among my colleagues, my patients and my friends. I have even seen the revolution in my own family. A few years ago, when I told my mother I was investigating the topic for a documentary, I was met with a long pause. \"Marijuana...?\" She whispered in a half questioning, half disapproving tone. She could barely even say the word and her response filled me with self-doubt. Even as a grown man, mom can still make my cheeks turn red and shatter my confidence with a single word. But just last week she suddenly stopped mid-conversation and said, \"I am proud of you on the whole marijuana thing.\" I waited for the other shoe to drop, but it didn't. Instead, she added, \"You probably helped a lot of people who were suffering.\" I don't think we had ever had a conversation like that one. At that moment, I saw a revolution that can bring you to tears. The word revolution, comes from the Latin revolutio, to \"turn around.\" I had my own turn around a couple of years ago, and at the time it was a lonely place to hold a supportive position on medical marijuana. Hardly any government officials would agree to sit down and be interviewed on the topic. Even patients I spoke to were reluctant to share their stories. It can be tricky, I learned, to be on the right side of science but on the wrong side of ideology. When we put the first \"Weed\" documentary on television in August 2013, I didn't know if anyone would watch our yearlong investigation. Even worse, I didn't even know if they would care. Is weed legal in your state? Just two years later, in \"Weed 3,\" we are eyewitnesses to a revolution in full swing. You will ride along with us for the dawn of the first federally approved clinical study on the use of marijuana for PTSD. You will meet patients such as Sean Kiernan, an accomplished investment banker, and Amelia Taylor, a stay-at-home mom. They are the remarkable and surprising faces of this revolution -- smart, successful and suffering -- unwilling to accept the fact that commonly prescribed medications often used to treat PTSD can be worse than the underlying disorder itself. Sean Kiernan nearly died, trying to get better. You will see what weed really does to your brain, in crystal clear images. This time around, you will hear from the heads of government agencies earnestly sharing their point of view, both Democratic and Republican senators, and even the President of the United States. This is what a revolution looks like. Your medical marijuana questions answered . When \"Weed 2: Cannabis Madness\" aired in March 2014, Boston researcher Rick Doblin believed the right people were watching. Just four days later, Doblin received a letter in the mail he had been waiting on for seven years that finally provided federal approval for his marijuana study. The federal farm where Doblin would have to obtain his marijuana is on the campus of Ole Miss in Oxford, Mississippi. In anticipation of a scientific revolution, the production of research-grade marijuana there has increased 30-fold in just the past year. Make no mistake, we have plenty of evidence that the approval and support of the federal government can fast track a revolution at a faster pace than we have yet seen. It was the National Institute of Allergy and Infectious Diseases that spearheaded the research into a cure for AIDS, as well as stopping the spread of West Nile Virus. They were also responsible for the awesome task of eradicating polio and smallpox. Other successful federally backed programs include the human genome project, the BRAIN initiative and the Precision Medicine Initiative. There are no shortage of examples where the federal government has been a guardian of our public health needs, and you could argue that medical marijuana would also qualify as a worthwhile investment. 10 diseases where medical marijuana could have impact . There is now promising research into the use of marijuana that could impact tens of thousands of children and adults, including treatment for cancer, epilepsy and Alzheimer's, to name a few. With regard to pain alone, marijuana could greatly reduce the demand for narcotics and simultaneously decrease the number of accidental painkiller overdoses, which are the greatest cause of preventable death in this country. As I sat across from Sens. Kirsten Gillibrand (D-New York) and Cory Booker (D-New Jersey), I knew something extraordinary was happening. They were reciting the story of Charlotte Figi and countless other children. They were quoting back the data we had shared from our earlier investigations. They were extolling the potential virtues of the plant, and all of that was before the interview even started. There was an impatience about them, and they seemed in a hurry to make a large dent in marijuana reform. They want marijuana to be rescheduled. They want it now. They want doctors to be able to prescribe it at VA hospitals all over the country. They want it now. They want research dollars freed up to study the plant. They want it now. They want their fellow lawmakers at the state and national level to acknowledge what most of the world, including the citizens of the United States, have known for a long time: Marijuana is a medicine, that should be studied and treated like any other medicine. And they want all of it now. I spent much of our interview challenging them. I needed to remind them that people, long before me or them, have been trying to do many of these same things for 40 years, and had been rejected every time. I reminded them that politicians have a hard time winning elections on the issue of marijuana but less difficulty losing them. I challenged them every step of the way. \"This time will be different,\" Booker confidently told me as he walked out of the room. Is marijuana as safe as -- or safer than -- alcohol? I know how easy it is do nothing because I did nothing for too long. Take a good look at the data, educate yourself and talk to the patients, who are often out of options and find their hope in the form of a simple plant. Journalists shouldn't take a position. It makes sense. Objectivity is king. But, at some point, open questions do get answered. At some point, contentious issues do get resolved. At some point, common sense prevails. So, here it is: We should legalize medical marijuana. We should do it nationally. And, we should do it now. 9 things to know about legal pot .\n"]},{"output_type":"stream","name":"stderr","text":["Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"min_length\": 56,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["\n","GENERATED SUMMARY:\n","CNN's John Sutter sees signs of a medical marijuana revolution everywhere...\"I see a revolution that is burning white hot among young people... among the parents and grandparents in my kids' school.. \"I see it in the faces of good parents uprooting their lives to get medicine for their children.\" Support for legalization has risen 11 points in the past few years alone.. in 1969, the first time Pew asked the question about legalization, only 12% of the nation was in favor..\n","\n","REFERENCE HIGHLIGHTS:\n","CNN's Dr. Sanjay Gupta says we should legalize medical marijuana now .\n","He says he knows how easy it is do nothing \"because I did nothing for too long\"\n","\n","--- SAMPLE 2 ---\n","\n","ARTICLE:\n","He looks barely teenage. But this child has amassed thousands of Twitter followers with his pictorial updates of 'gang life'. The baby-faced boy from Memphis, Tennessee, poses with guns, cash, and bags of what looks like marijuana. Scroll down for video . Baby-faced: This little boy has amassed more than 3,000 followers on Twitter with pictures like these . In many pictures he is smoking suspicious substances, with captions such as 'High Life' Backlash: The boy, from Memphis, has prompted a wave of critics calling his stunts 'sad' In one video he laughs and points the gun at the camera in an apparent attempt to look menacing - as adults laugh in the background. In others, he is pictured blowing smoke, with the caption: 'High Life'. Tweets include the phrases, 'I need a bad b****', 'f*** da police', and 'gang sh** n****'. Some feature sexual references, such as: 'quit suckin my d***'. As he is a minor, DailyMail.com will not identify the little boy. The child's tweets have prompted backlash from other Twitter users and members of the community. One woman tweeted him: 'you need help i feel so bad for you your parents should be ashamed please get help and a education this is sad and disappointing.' Another Twitter user said: 'You poor baby. Just stop it & go to school.' Adults around: Many of his pictures and videos are taken with adults who are seen laughing, unfazed . Guns: The little boy, who appears to be barely teenage, regularly poses for his followers with guns . The boy hit back at his critics claiming he has had a tough year and the backlash is 'stressing out' his mother . Orrden Williams, a Memphis resident who has been the victim of gang violence, told AOL.com: 'What he's doing on here is something that he's going to bring upon society one day. It's just inevitable.' He added: 'All it takes is the right influence, and all this can change. So that's a wake-up Memphis call right there.' Hitting back at critics, the boy tweeted on Friday: 'Wish some people would mind dey own business' as 'it's stressing my mama out'. He adds that his sister has 'been locked up for two years' and his critics 'don't know the half of what we been through end of 2014 & beg of 2015.' Authorities are increasingly cognizant of Twitter as a means of perpetuating gang culture. While many like this young boy use it as a forum to flaunt their drugs and guns, around 50 per cent of organized gangs maintain in the United States maintain a social media profile.\n"]},{"output_type":"stream","name":"stderr","text":["Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_bos_token_id\": 0,\n","  \"forced_eos_token_id\": 2,\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"min_length\": 56,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["\n","GENERATED SUMMARY:\n","Baby-faced boy from Memphis, Tennessee, posts pictures of 'gang life' with guns, cash and marijuana. In many pictures he is smoking suspicious substances, with captions such as 'High Life'. Critics have called his stunts'sad' and 'disappointing'. The boy hit back at critics claiming he has had a tough year and the backlash is'stressing out' his mother.\n","\n","REFERENCE HIGHLIGHTS:\n","Child has amassed thousands of Twitter followers with 'gang life' photos .\n","In one video he points gun at camera as adults look on unfazed .\n","His tweets have prompted backlash with calls for intervention .\n","\n","--- SAMPLE 3 ---\n","\n","ARTICLE:\n","New Jersey Governor Chris Christie wasn't looking too presidential Tuesday night when he got into a heated debate with a veteran teacher at a town hall meeting. And now the state's largest teacher's union is calling him out for his 'bullying' behavior. 'He's always taken a very nasty and disrespectful tone with teachers and other individuals who dare to question him at these events,' Steve Wollmer of the NJ Education Association told NJ.com. 'It's the one thing that never seems to change.' Scroll Down for Video . Not being nice: New Jersey Gov Chris Christie (left) is being called a bully for the way he interacted with a teacher (Kathy Mooney, right) at a Tuesday night town hall meeting . That sentiment doesn't ring well with Christie's ambitions to run in the Republican presidential primaries next year. Tuesday night, Christie appeared at Kenilworth Town Hall to take questions from a group of citizens, when Kathy Mooney, a high school English Teacher from Roselle Park, took the microphone. Ms Mooney, who has been a teacher for 27 years, questioned Christie's motivations behind a legal settlement with oil company ExxonMobil which could have contributed drastically to the state's pension plans for teachers. Christie settled the deal for $225million, despite the fact that the state had originally asked for $8.9billion which Mooney described as 'favoring the affluent' and 'kicking state workers under the bus'. 'I know that you could have gotten more money, on the dollar,' Mooney said. 'Do you?' a defensive Christie quickly responded. 'You do know that? Really? You know that?' Mooney started to respond, but not before being cut off by the governor. A good deal? Mooney questioned Christie's decision-making behind a $225million legal settlement. The state originally wanted $8.9billion from the oil company, and Mooney says that would have had a huge impact on pensions for public employees . 'I mean: Do you know that?' Christie continued. 'I wanna know how you know that. Because you're a teacher, and you're standing in front of students every day, conveying to them, facts - things that they need to learn. So I would like to understand your analysis of how you know that in a ten year long, court case, that you have enough detail to know.' Mooney explains that she read about the deal in the newspaper and did not attend the meeting 'to be bullied'. 'You're not being bullied, because you're asking me a question, I'm going to ask you questions back,' Christie says, as he continues to dodge the issue. 'He said, \"I'm not bullying you' as he bullied her,\"' Wollmer said of the exchange. Perhaps the reason why Wollmer and his union responded sharply to Christie's town hall meeting Tuesday night, is that he blamed the union for their role in the current pension system. 'The fact is your union, over the course of time, has asked for significantly higher benefits - more expensive benefits - that your union knew the state could not afford,' Christie said Tuesday.\n","\n","GENERATED SUMMARY:\n","New Jersey Gov Chris Christie got into a heated debate with a high school English teacher at a town hall meeting Tuesday night...\"He's always taken a very nasty and disrespectful tone with teachers and other individuals who dare to question him at these events...The state's largest teacher's union is calling Christie out for his 'bullying' behavior...)Christie settled a legal settlement with oil company ExxonMobil for $225million..,\"Kathy Mooney questioned Christie's motivations behind the deal.\".\"The state originally wanted $8.9billion.\"The NJ Education Association says Christie 'never seems to change'..\"It's the one thing that never appears to change..\"Christie's ambitions to run in the Republican presidential primaries next year.\"The New Jersey Education Association calls Christie's behavior 'disrespectful'\n","\n","REFERENCE HIGHLIGHTS:\n","The presidential hopeful held a town hall meeting in Kenilworth on Tuesday .\n","During the meeting, high school English teacher Kathy Mooney got up to ask the governor a question about pensions .\n","She asked why he didn't seek a higher legal settlement in a case with ExxonMobil that would have contributed to the state's pension system .\n","Christie responded by repeatedly asking how much Mooney knew about the deal instead of answering her question .\n"]}]},{"cell_type":"code","source":["# @title PPO\n","# pip install transformers datasets evaluate tqdm nltk trl accelerate\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Silence excessive logs\n","import logging\n","from transformers import logging as hf_logging\n","import datasets, evaluate\n","hf_logging.set_verbosity_error()\n","datasets.logging.set_verbosity_error()\n","evaluate.logging.set_verbosity_error()\n","logging.getLogger(\"nltk\").setLevel(logging.ERROR)\n","\n","import torch\n","from datasets import load_dataset, DatasetDict\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSeq2SeqLM,\n","    DataCollatorForSeq2Seq,\n","    Seq2SeqTrainingArguments,\n","    Seq2SeqTrainer,\n","    AutoModelForSequenceClassification\n",")\n","from trl import Seq2SeqPPOConfig, Seq2SeqPPOTrainer\n","\n","# ========== CONFIGURATION ===========\n","MODEL_NAME        = \"facebook/bart-large-cnn\"\n","OUTPUT_DIR        = \"./bart_cnn_sum\"\n","BATCH_SIZE        = 8\n","NUM_EPOCHS        = 3\n","MAX_INPUT_LENGTH  = 512\n","MAX_OUTPUT_LENGTH = 142\n","GEN_MAX_LENGTH    = MAX_OUTPUT_LENGTH * 2\n","\n","TRAIN_SIZE = None   # e.g. 1000 or None\n","VAL_SIZE   = None\n","TEST_SIZE  = None\n","\n","LOG_STEPS        = 50\n","EVAL_STEPS       = 200\n","SAVE_STEPS       = 200\n","SAVE_TOTAL_LIMIT = 3\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {DEVICE}\")\n","\n","# ========== SUPERVISED FINE-TUNING ==========\n","# Load and tokenize CNN/DailyMail dataset\n","raw = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n","def sample_split(split, n):\n","    ds = raw[split]\n","    return ds.shuffle(seed=42).select(range(n)) if n is not None else ds\n","\n","data = DatasetDict({\n","    \"train\":      sample_split(\"train\", TRAIN_SIZE),\n","    \"validation\": sample_split(\"validation\", VAL_SIZE),\n","    \"test\":       sample_split(\"test\", TEST_SIZE),\n","})\n","print(f\"Train={len(data['train'])}, Val={len(data['validation'])}, Test={len(data['test'])}\")\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","def preprocess(batch):\n","    inputs = tokenizer(batch[\"article\"], max_length=MAX_INPUT_LENGTH, truncation=True, padding=\"max_length\")\n","    labels = tokenizer(batch[\"highlights\"], max_length=MAX_OUTPUT_LENGTH, truncation=True, padding=\"max_length\").input_ids\n","    inputs[\"labels\"] = labels\n","    return inputs\n","\n","tokenized = data.map(preprocess, batched=True, num_proc=4, remove_columns=[\"article\",\"highlights\"])\n","\n","# Supervised training\n","model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n","\n","train_args = Seq2SeqTrainingArguments(\n","    output_dir=OUTPUT_DIR,\n","    num_train_epochs=NUM_EPOCHS,\n","    per_device_train_batch_size=BATCH_SIZE,\n","    per_device_eval_batch_size=BATCH_SIZE,\n","    logging_steps=LOG_STEPS,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=EVAL_STEPS,\n","    save_strategy=\"steps\",\n","    save_steps=SAVE_STEPS,\n","    save_total_limit=SAVE_TOTAL_LIMIT,\n","    predict_with_generate=True,\n","    fp16=(DEVICE.type==\"cuda\"),\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"rougeL\",\n","    greater_is_better=True,\n","    report_to=\"none\",\n",")\n","\n","evaluator = evaluate.load(\"rouge\")\n","def compute_sft_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    preds = preds[0] if isinstance(preds, tuple) else preds\n","    dec_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    labels = torch.where(torch.tensor(labels) != -100, torch.tensor(labels), tokenizer.pad_token_id).tolist()\n","    dec_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    return evaluator.compute(predictions=dec_preds, references=dec_labels, use_stemmer=True)\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=train_args,\n","    train_dataset=tokenized[\"train\"],\n","    eval_dataset=tokenized[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n","    compute_metrics=compute_sft_metrics,\n",")\n","\n","print(\"▶ Starting supervised fine-tuning...\")\n","trainer.train()\n","\n","# ========== RLHF WITH PPO ==========\n","# Load reward model trained on human preferences\n","reward_model = AutoModelForSequenceClassification.from_pretrained(\"openai/reward-model\").to(DEVICE)\n","reward_model.eval()\n","\n","# Convert policy to seq2seq PPO trainer\n","ppo_config = Seq2SeqPPOConfig(\n","    model_name=MODEL_NAME,\n","    learning_rate=1e-5,\n","    batch_size=BATCH_SIZE,\n","    ppo_epochs=1,\n",")\n","\n","ppo_trainer = Seq2SeqPPOTrainer(\n","    ppo_config,\n","    policy_model=model,\n","    ref_model=None,  # optional reference model for KL control\n","    tokenizer=tokenizer,\n","    dataset=data[\"train\"],\n","    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",")\n","\n","# PPO loop over a small subset\n","for sample in data[\"train\"].shuffle(seed=0).select(range(100)):\n","    # Prepare inputs\n","    query = tokenizer(sample[\"article\"], return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n","    # Generate response\n","    response_tokens = ppo_trainer.generate(query[\"input_ids\"], max_length=GEN_MAX_LENGTH, num_beams=4)\n","\n","    # Compute reward\n","    # Reward model expects concatenated input + response\n","    concat = tokenizer(\n","        sample[\"article\"],\n","        tokenizer.decode(response_tokens[0], skip_special_tokens=True),\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        padding=True\n","    ).to(DEVICE)\n","    reward = reward_model(**concat).logits.squeeze().detach()\n","\n","    # Run PPO step\n","    ppo_trainer.step(query[\"input_ids\"], response_tokens, reward)\n","\n","print(\"▶ PPO training completed.\")\n","\n","# ========== EVALUATION ==========\n","print(\"== Final evaluation on test set ==\")\n","metrics = ppo_trainer.evaluate(tokenized[\"test\"])\n","print(metrics)\n","\n","# Generate examples\n","for i in range(5):\n","    art = data[\"test\"][i][\"article\"]\n","    print(f\"\\n--- Example {i+1} ---\")\n","    print(\"ARTICLE:\", art)\n","    inp = tokenizer(art, return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n","    with torch.no_grad():\n","        out = model.generate(**inp, max_length=GEN_MAX_LENGTH, num_beams=4)\n","    summary = tokenizer.decode(out[0], skip_special_tokens=True)\n","    print(\"SUMMARY:\", summary)\n","    print(\"HIGHLIGHTS:\", data[\"test\"][i][\"highlights\"])\n"],"metadata":{"cellView":"form","id":"IcfRY3F6mrvk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Uninstall any mismatched versions\n","!pip uninstall -y transformers huggingface_hub trl accelerate\n","\n","# Freshly install all essentials in one go\n","!pip install --upgrade \\\n","    datasets \\\n","    transformers \\\n","    accelerate \\\n","    huggingface_hub \\\n","    evaluate \\\n","    trl\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Wn-MJ08oOO6","executionInfo":{"status":"ok","timestamp":1746402820123,"user_tz":300,"elapsed":13674,"user":{"displayName":"Akhil Mavallapalli","userId":"02508402536572482827"}},"outputId":"b3a433cb-7e12-4086-d004-627a41bee59d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mFound existing installation: huggingface-hub 0.30.2\n","Uninstalling huggingface-hub-0.30.2:\n","  Successfully uninstalled huggingface-hub-0.30.2\n","\u001b[33mWARNING: Skipping trl as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mFound existing installation: accelerate 1.6.0\n","Uninstalling accelerate-1.6.0:\n","  Successfully uninstalled accelerate-1.6.0\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.7)\n","Collecting datasets\n","  Using cached datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n","Collecting transformers\n","  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n","Collecting accelerate\n","  Using cached accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n","Collecting huggingface_hub\n","  Using cached huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.4.dev0)\n","Collecting trl\n","  Using cached trl-0.17.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.14)\n","Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2023.10.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Collecting tokenizers<0.22,>=0.21 (from transformers)\n","  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.7.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (13.9.4)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.5.1.17)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.3)\n","Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.26.2)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n","Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.3.0)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch>=2.0.0->accelerate) (75.2.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.19.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n","Using cached datasets-3.5.1-py3-none-any.whl (491 kB)\n","Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hUsing cached accelerate-1.6.0-py3-none-any.whl (354 kB)\n","Using cached huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n","Using cached trl-0.17.0-py3-none-any.whl (348 kB)\n","Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: huggingface_hub, tokenizers, transformers, datasets, accelerate, trl\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.15.2\n","    Uninstalling tokenizers-0.15.2:\n","      Successfully uninstalled tokenizers-0.15.2\n","  Attempting uninstall: datasets\n","    Found existing installation: datasets 2.14.7\n","    Uninstalling datasets-2.14.7:\n","      Successfully uninstalled datasets-2.14.7\n","Successfully installed accelerate-1.6.0 datasets-3.5.1 huggingface_hub-0.30.2 tokenizers-0.21.1 transformers-4.51.3 trl-0.17.0\n"]}]},{"cell_type":"code","source":["import datasets, transformers, accelerate, huggingface_hub, evaluate, trl\n","print(\"OK:\", datasets.__version__, transformers.__version__, accelerate.__version__, huggingface_hub.__version__, evaluate.__version__, trl.__version__)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"id":"TusMXEOkuCts","executionInfo":{"status":"error","timestamp":1746402829505,"user_tz":300,"elapsed":3465,"user":{"displayName":"Akhil Mavallapalli","userId":"02508402536572482827"}},"outputId":"adb0d5fe-de66-4756-8f2d-ff58d53e69bb"},"execution_count":3,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"operator torchvision::nms does not exist","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-2a3abe7b7da0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccelerate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuggingface_hub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OK:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccelerate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuggingface_hub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/evaluate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mevaluation_suite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvaluationSuite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m from .evaluator import (\n\u001b[1;32m     31\u001b[0m     \u001b[0mAudioClassificationEvaluator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/evaluate/evaluation_suite/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloading\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluation_module_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/evaluate/evaluator/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipelines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSUPPORTED_TASKS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSUPPORTED_PIPELINE_TASKS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipelines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTASK_ALIASES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipelines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_task\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcheck_pipeline_task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_module_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_class_from_dynamic_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedFeatureExtractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_processing_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseImageProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration_auto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction_auto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFEATURE_EXTRACTOR_MAPPING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoFeatureExtractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_processing_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageProcessingMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_transforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcenter_crop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrescale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChannelDimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_image_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m from .image_utils import (\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mChannelDimension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mImageInput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_torchvision_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtorchvision_io\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpolationMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_custom_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl_abstract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchvision::nms\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmeta_nms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"boxes should be a 2d tensor, got {dets.dim()}D\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/library.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0muse_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m         \u001b[0muse_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_fake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/library.py\u001b[0m in \u001b[0;36m_register_fake\u001b[0;34m(self, op_name, fn, _stacklevel)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mfunc_to_register\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_to_register\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registration_handles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_library/fake_impl.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, func, source)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;34mf\"{self.kernel.source}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             )\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_has_kernel_for_dispatch_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqualname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             raise RuntimeError(\n\u001b[1;32m     33\u001b[0m                 \u001b[0;34mf\"register_fake(...): the operator {self.qualname} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: operator torchvision::nms does not exist"]}]},{"cell_type":"code","source":["# Install compatible versions (upgrade to latest if needed)\n","# !pip install --upgrade transformers huggingface_hub trl accelerate\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Silence library logs\n","import logging\n","from transformers import logging as hf_logging\n","import datasets, evaluate\n","hf_logging.set_verbosity_error()\n","datasets.logging.set_verbosity_error()\n","evaluate.logging.set_verbosity_error()\n","logging.getLogger(\"nltk\").setLevel(logging.ERROR)\n","\n","import torch\n","import numpy as np\n","from datasets import load_dataset, DatasetDict\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSeq2SeqLM,\n","    DataCollatorForSeq2Seq,\n","    Seq2SeqTrainingArguments,\n","    Seq2SeqTrainer,\n","    AutoModelForSequenceClassification\n",")\n","from trl import Seq2SeqPPOConfig, Seq2SeqPPOTrainer\n","\n","# ========== CONFIGURATION ===========\n","MODEL_NAME         = \"facebook/bart-large-cnn\"\n","OUTPUT_DIR         = \"./bart_cnn_sum\"\n","BATCH_SIZE         = 16\n","NUM_EPOCHS         = 3\n","MAX_INPUT_LENGTH   = 512\n","MAX_OUTPUT_LENGTH  = 64\n","\n","# Dynamic generation lengths will be set after supervised training\n","\n","TRAIN_SIZE = 20  # e.g. 1000 or None for full split\n","VAL_SIZE   = 3\n","TEST_SIZE  = 3\n","\n","LOG_STEPS        = 50\n","EVAL_STEPS       = 200\n","SAVE_STEPS       = 200\n","SAVE_TOTAL_LIMIT = 3\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {DEVICE}\")\n","\n","# ========== LOAD & SAMPLE DATA ===========\n","raw = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n","def sample_split(split, n):\n","    ds = raw[split]\n","    return ds.shuffle(seed=42).select(range(n)) if n is not None else ds\n","\n","data = DatasetDict({\n","    \"train\":      sample_split(\"train\",    TRAIN_SIZE),\n","    \"validation\": sample_split(\"validation\", VAL_SIZE),\n","    \"test\":       sample_split(\"test\",      TEST_SIZE),\n","})\n","print(f\"Dataset sizes — train: {len(data['train'])}, val: {len(data['validation'])}, test: {len(data['test'])}\")\n","\n","# ========== TOKENIZER & PREPROCESS ===========\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","def preprocess(batch):\n","    inp = tokenizer(batch[\"article\"], max_length=MAX_INPUT_LENGTH, truncation=True, padding=\"max_length\")\n","    lbl = tokenizer(batch[\"highlights\"], max_length=MAX_OUTPUT_LENGTH, truncation=True, padding=\"max_length\").input_ids\n","    inp[\"labels\"] = lbl\n","    return inp\n","tokenized = data.map(preprocess, batched=True, num_proc=4, remove_columns=[\"article\",\"highlights\"])\n","\n","# ========== SUPERVISED FINE-TUNING ===========\n","model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n","train_args = Seq2SeqTrainingArguments(\n","    output_dir                 = OUTPUT_DIR,\n","    num_train_epochs           = NUM_EPOCHS,\n","    per_device_train_batch_size= BATCH_SIZE,\n","    per_device_eval_batch_size = BATCH_SIZE,\n","    logging_strategy           = \"steps\",\n","    logging_steps              = LOG_STEPS,\n","    evaluation_strategy        = \"steps\",\n","    eval_steps                 = EVAL_STEPS,\n","    save_strategy              = \"steps\",\n","    save_steps                 = SAVE_STEPS,\n","    save_total_limit           = SAVE_TOTAL_LIMIT,\n","    predict_with_generate      = True,\n","    fp16                       = (DEVICE.type == \"cuda\"),\n","    load_best_model_at_end     = True,\n","    metric_for_best_model      = \"rougeL\",\n","    greater_is_better          = True,\n","    report_to                  = \"none\",\n",")\n","evaluator = evaluate.load(\"rouge\")\n","\n","def compute_sft_metrics(preds_labels):\n","    preds, labels = preds_labels\n","    if isinstance(preds, tuple): preds = preds[0]\n","    decoded_preds  = tokenizer.batch_decode(preds,  skip_special_tokens=True)\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    return evaluator.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","\n","trainer = Seq2SeqTrainer(\n","    model           = model,\n","    args            = train_args,\n","    train_dataset   = tokenized[\"train\"],\n","    eval_dataset    = tokenized[\"validation\"],\n","    tokenizer       = tokenizer,\n","    data_collator   = DataCollatorForSeq2Seq(tokenizer, model=model),\n","    compute_metrics = compute_sft_metrics,\n",")\n","print(\"▶ Starting supervised fine-tuning…\")\n","trainer.train()\n","\n","# ========== DYNAMIC GENERATION LENGTH SETUP ===========\n","highlight_lens = [len(tokenizer.tokenize(h)) for h in raw[\"train\"][\"highlights\"]]\n","avg_highlight_len = int(sum(highlight_lens)/len(highlight_lens))\n","min_length = avg_highlight_len\n","max_length = avg_highlight_len * 2\n","print(f\"Avg highlight length = {avg_highlight_len}; generation min={min_length}, max={max_length}\")\n","\n","# ========== RLHF + PPO ===========\n","# Load reward model\n","reward_model = AutoModelForSequenceClassification.from_pretrained(\"openai/reward-model\").to(DEVICE)\n","reward_model.eval()\n","\n","# PPO configuration\n","ppo_config = Seq2SeqPPOConfig(\n","    model_name  = MODEL_NAME,\n","    learning_rate=1e-5,\n","    batch_size   = BATCH_SIZE,\n","    ppo_epochs   = 1,\n",")\n","ppo_trainer = Seq2SeqPPOTrainer(\n","    ppo_config,\n","    policy_model = model,\n","    ref_model    = None,\n","    tokenizer    = tokenizer,\n","    dataset      = data[\"train\"],\n","    data_collator= DataCollatorForSeq2Seq(tokenizer, model=model),\n",")\n","\n","# PPO loop on subset\n","for sample in data[\"train\"].shuffle(seed=0).select(range(100)):\n","    query = tokenizer(sample[\"article\"], return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n","    response = ppo_trainer.generate(\n","        query[\"input_ids\"],\n","        min_length=min_length,\n","        max_length=max_length,\n","        num_beams=5,\n","        length_penalty=1.1,\n","        early_stopping=True\n","    )\n","    concat_inputs = tokenizer(\n","        sample[\"article\"],\n","        tokenizer.decode(response[0], skip_special_tokens=True),\n","        return_tensors=\"pt\", truncation=True, padding=True\n","    ).to(DEVICE)\n","    reward = reward_model(**concat_inputs).logits.squeeze().detach()\n","    ppo_trainer.step(query[\"input_ids\"], response, reward)\n","print(\" PPO training completed.\")\n","\n","# ========== EVALUATION & EXAMPLE GENERATION ===========\n","print(\"== Validation / Test Metrics ==\")\n","print(ppo_trainer.evaluate(tokenized[\"validation\"]))\n","print(ppo_trainer.evaluate(tokenized[\"test\"]))\n","\n","print(\"\\n== Example Generations ==\")\n","for i, sample in enumerate(data[\"test\"][:5]):\n","    print(f\"--- Sample {i+1} ---\")\n","    print(\"ARTICLE:\", sample[\"article\"])\n","    inp = tokenizer(sample[\"article\"], return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n","    with torch.no_grad():\n","        out = model.generate(\n","            **inp,\n","            min_length=min_length,\n","            max_length=max_length,\n","            num_beams=5,\n","            length_penalty=1.1,\n","            early_stopping=True\n","        )\n","    print(\"SUMMARY:\", tokenizer.decode(out[0], skip_special_tokens=True))\n","    print(\"HIGHLIGHTS:\", sample[\"highlights\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"33uPyQgcm4V8","executionInfo":{"status":"error","timestamp":1746402747506,"user_tz":300,"elapsed":57,"user":{"displayName":"Akhil Mavallapalli","userId":"02508402536572482827"}},"outputId":"62128178-2c74-4615-fb72-d1170bc31f78"},"execution_count":1,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'transformers'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-f76f4e21a60f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Silence library logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhf_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mhf_logging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]}]}